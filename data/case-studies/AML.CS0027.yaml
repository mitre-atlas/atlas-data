---
id: AML.CS0027
name: Organization Confusion on Hugging Face
object-type: case-study
summary: '[threlfall_hax](https://5stars217.github.io/), a security researcher, created
  organization accounts on Hugging Face, a public model repository, that impersonated
  real organizations. These false Hugging Face organization accounts looked legitimate
  so individuals from the impersonated organizations requested to join, believing
  the accounts to be an official site for employees to share models. This gave the
  researcher full access to any AI models uploaded by the employees, including the
  ability to replace models with malicious versions. The researcher demonstrated that
  they could embed malware into an AI model that provided them access to the victim
  organization''s environment. From there, threat actors could execute a range of
  damaging attacks such as intellectual property theft or poisoning other AI models
  within the victim''s environment.'
incident-date: 2023-08-23
incident-date-granularity: DATE
procedure:
- tactic: '{{resource_development.id}}'
  technique: '{{establish_accounts.id}}'
  description: The researcher registered an unverified "organization" account on Hugging
    Face that squats on the namespace of a targeted company.
- tactic: '{{defense_evasion.id}}'
  technique: '{{impersonation.id}}'
  description: Employees of the targeted company found and joined the fake Hugging
    Face organization. Since the organization account name is matches or appears to
    match the real organization, the employees were fooled into believing the account
    was official.
- tactic: '{{ml_model_access.id}}'
  technique: '{{full_access.id}}'
  description: The employees made use of the Hugging Face organizaion and uploaded
    private models. As owner of the Hugging Face account, the researcher has full
    read and write access to all of these uploaded models.
- tactic: '{{impact.id}}'
  technique: '{{ip_theft.id}}'
  description: With full access to the model, an adversary could steal valuable intellectual
    property in the form of AI models.
- tactic: '{{ml_attack_staging.id}}'
  technique: '{{embed_malware.id}}'
  description: The researcher embedded [Sliver](https://github.com/BishopFox/sliver),
    an open source C2 server, into the target model. They added a `Lambda` layer to
    the model, which allows for arbitrary code to be run, and used an `exec()` call
    to execute the Sliver payload.
- tactic: '{{resource_development.id}}'
  technique: '{{publish_poisoned_model.id}}'
  description: The researcher re-uploaded the manipulated model to the Hugging Face
    repository.
- tactic: '{{initial_access.id}}'
  technique: '{{supply_chain_model.id}}'
  description: The victim's AI model supply chain is now compromised. Users of the
    model repository will receive the adversary's model with embedded malware.
- tactic: '{{execution.id}}'
  technique: '{{unsafe_ml_artifacts.id}}'
  description: When any future user loads the model, the model automatically executes
    the adversary's payload.
- tactic: '{{defense_evasion.id}}'
  technique: '{{masquerading.id}}'
  description: The researcher named the Sliver process `training.bin` to disguise
    it as a legitimate model training process. Furthermore, the model still operates
    as normal, making it less likely a user will notice something is wrong.
- tactic: '{{command_and_control.id}}'
  technique: '{{reverse_shell.id}}'
  description: The Silver implant grants the researcher a command and control channel
    so they can explore the victim's environment and continue the attack.
- tactic: '{{credential_access.id}}'
  technique: '{{unsecured_credentials.id}}'
  description: The researcher checked environment variables and searched Jupyter notebooks
    for API keys and other secrets.
- tactic: '{{exfiltration.id}}'
  technique: '{{exfiltrate_via_cyber.id}}'
  description: Discovered credentials could be exfiltrated via the Sliver implant.
- tactic: '{{discovery.id}}'
  technique: '{{discover_ml_artifacts.id}}'
  description: The researcher could have searched for AI models in the victim organization's
    environment.
- tactic: '{{resource_development.id}}'
  technique: '{{obtain_advml.id}}'
  description: The researcher obtained [EasyEdit](https://github.com/zjunlp/EasyEdit),
    an open-source knowledge editing tool for large language models.
- tactic: '{{ml_attack_staging.id}}'
  technique: '{{poison_model.id}}'
  description: The researcher demonstrated that EasyEdit could be used to poison a
    `Llama-2-7-b` with false facts.
- tactic: '{{impact.id}}'
  technique: '{{external_harms.id}}'
  description: If the company's models were manipulated to produce false information,
    a variety of harms including financial and reputational could occur.
target: Hugging Face users
actor: threlfall_hax
case-study-type: exercise
references:
- title: Model Confusion - Weaponizing ML models for red teams and bounty hunters
  url: https://5stars217.github.io/2023-08-08-red-teaming-with-ml-models/#unexpected-benefits---organization-confusion
