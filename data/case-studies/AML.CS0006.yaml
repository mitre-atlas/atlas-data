---
id: AML.CS0006
name: ClearviewAI Misconfiguration
object-type: case-study
summary: 'Clearview AI''s source code repository, though password protected, was misconfigured
  to allow an arbitrary user to register an account.

  This allowed an external researcher to gain access to a private code repository
  that contained Clearview AI production credentials, keys to cloud storage buckets
  containing 70K video samples, and copies of its applications and Slack tokens.

  With access to training data, a bad-actor has the ability to cause an arbitrary
  misclassification in the deployed model.

  These kinds of attacks illustrate that any attempt to secure ML system should be
  on top of "traditional" good cybersecurity hygiene such as locking down the system
  with least privileges, multi-factor authentication and monitoring and auditing.

  '
incident-date: 2020-04-16
incident-date-granularity: DATE
procedure:
- tactic: '{{initial_access.id}}'
  technique: '{{valid_accounts.id}}'
  description: 'In this scenario, a security researcher gained initial access to via
    a valid account that was created through a misconfiguration.

    '
reported-by: Mossab Hussein (@mossab_hussein)
references:
- title: TechCrunch Article, "Security lapse exposed Clearview AI source code"
  url: https://techcrunch.com/2020/04/16/clearview-source-code-lapse/amp/
- title: Gizmodo Article, "We Found Clearview AI's Shady Face Recognition App"
  url: https://gizmodo.com/we-found-clearview-ais-shady-face-recognition-app-1841961772
