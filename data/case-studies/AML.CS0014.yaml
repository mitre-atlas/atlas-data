---
id: AML.CS0014
name: Kaspersky - Confusing Antimalware Neural Networks
object-type: case-study
summary: 'Cloud storage and computations have become popular platforms for deploying
  ML malware detectors.

  In such cases, the features for models are built on users'' systems and then sent
  to cybersecurity company servers.

  The Kaspersky ML research team explored this gray-box scenario and shown that feature
  knowledge is enough for an adversarial attack on ML models.


  They attacked one of Kaspersky''s antimalware ML models without white-box access
  to it and successfully evaded detection for most of the adversarially modified malware
  files.

  '
incident-date: 2021-06-23
incident-date-granularity: DATE
procedure:
- tactic: '{{reconnaissance.id}}'
  technique: '{{vuln_analysis.id}}'
  description: 'The researchers performed a review of adversarial ML attacks on antimalware
    products.

    They discovered that techniques borrowed from attacks on image classifiers have
    been successfully applied to the antimalware domain.

    However, it was not clear if these approaches were effective against the ML component
    of production antimalware solutions.

    '
- tactic: '{{reconnaissance.id}}'
  technique: '{{victim_website.id}}'
  description: 'Kaspersky''s use of ML-based antimalware detectors is publicly documented
    on their website. In practice, an adversary could use this for targeting.

    '
- tactic: '{{ml_model_access.id}}'
  technique: '{{ml_service.id}}'
  description: 'The researches used access to the target ML-based antimalware product
    throughout this case study.

    This product scans files on the user''s system, extracts features locally, then
    sends them to the cloud-based ML malware detector for classification.

    Therefore, the researchers had only black-box access to the malware detector itself,
    but could learn valuable information for constructing the attack from the feature
    extractor.

    '
- tactic: '{{resource_development.id}}'
  technique: '{{acquire_ml_artifacts_data.id}}'
  description: 'The researchers collected a dataset of malware and clean files.

    They scanned the dataset with the target ML-based antimalware solution and labeled
    the samples according the ML detector''s predictions.

    '
- tactic: '{{ml_attack_staging.id}}'
  technique: '{{train_proxy_model.id}}'
  description: 'Then, a proxy model was trained on the labeled dataset of malware
    and clean files.

    The researchers experimented with a variety of model architectures.

    '
- tactic: '{{resource_development.id}}'
  technique: '{{develop_advml.id}}'
  description: 'By reverse engineering the local feature extractor, the researchers
    could collect information about the input features, used for the cloud-based ML
    detector.

    The model collects PE Header features, section features and section data statistics,
    and file strings information.

    A gradient based adversarial algorithm for executable files was developed.

    The algorithm manipulates file features to avoid detection by the proxy model,
    while still containing the same malware payload

    '
- tactic: '{{ml_attack_staging.id}}'
  technique: '{{craft_adv_transfer.id}}'
  description: 'Using a developed gradient-driven algorithm, malicious adversarial
    files for the proxy model were constructed from the malware files for black-box
    transfer to the target model.

    '
- tactic: '{{ml_attack_staging.id}}'
  technique: '{{verify_attack.id}}'
  description: 'The adversarial malware files were tested against the target antimalware
    solution to verify their efficacy.

    '
- tactic: '{{defense_evasion.id}}'
  technique: '{{evade_model.id}}'
  description: 'The researchers demonstrated that for most of the adversarial files,
    the antimalware model was successfully evaded.

    In practice, an adversary could deploy their adversarially crafted malware and
    infect systems while evading detection.

    '
reported-by: 'Alexey Antonov and Alexey Kogtenkov (ML researchers, Kaspersky ML team) '
references:
- title: Article, "How to confuse antimalware neural networks. Adversarial attacks
    and protection"
  url: https://securelist.com/how-to-confuse-antimalware-neural-networks-adversarial-attacks-and-protection/102949/
