---
id: AML.CS0009
object-type: case-study
name: Tay Poisoning
summary: |
  Microsoft created Tay, a Twitter chatbot designed to engage and entertain users.
  While previous chatbots used pre-programmed scripts
  to respond to prompts, Tay's machine learning capabilities allowed it to be
  directly influenced by its conversations.

  A coordinated attack encouraged malicious users to tweet abusive and offensive language at Tay,
  which eventually led to Tay generating similarly inflammatory content towards other users.

  Microsoft decommissioned Tay within 24 hours of its launch and issued a public apology
  with lessons learned from the bot's failure.
incident-date: 2016-03-23
incident-date-granularity: DATE
procedure:
- tactic: '{{ml_model_access.id}}'
  technique: '{{ml_service.id}}'
  description: Adversaries were able to interact with Tay via Twitter messages.
- tactic: '{{initial_access.id}}'
  technique: '{{supply_chain_data.id}}'
  description: 'Tay bot used the interactions with its Twitter users as training data
    to improve its conversations.

    Adversaries were able to coordinate with the intent of defacing Tay bot by exploiting
    this feedback loop.'
- tactic: '{{persistence.id}}'
  technique: '{{poison_data.id}}'
  description: By repeatedly interacting with Tay using racist and offensive language,
    they were able to bias Tay's dataset towards that language as well. This was done
    by adversaries using the "repeat after me" function, a command that forced Tay
    to repeat anything said to it.
- tactic: '{{impact.id}}'
  technique: '{{erode_integrity.id}}'
  description: As a result of this coordinated attack, Tay's conversation algorithms
    began to learn to generate reprehensible material. Tay's internalization of
    this detestable language caused it to be unpromptedly repeated during interactions
    with innocent users.
reporter: Microsoft
target: Microsoft's Tay AI Chatbot
actor: 4chan Users
case-study-type: incident
references:
- title: Microsoft BlogPost, "Learning from Tay's introduction"
  url: https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/
- title: IEEE Article, "In 2016, Microsoft's Racist Chatbot Revealed the Dangers of
    Online Conversation"
  url: https://spectrum.ieee.org/tech-talk/artificial-intelligence/machine-learning/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation
