---
id: AML.CS0019
name: PoisonGPT
object-type: case-study
summary: Researchers from Mithril Security demonstrated how to poison an open-source
  pre-trained large language model (LLM) to return a false fact. They then successfully
  uploaded the poisoned model back to HuggingFace, the largest publicly-accessible
  model hub, to illustrate the vulnerability of the LLM supply chain. Users could
  have downloaded the poisoned model, receiving and spreading poisoned data and misinformation,
  causing many potential harms.
incident-date: 2023-07-01
incident-date-granularity: MONTH
procedure:
- tactic: '{{resource_development.id}}'
  technique: '{{acquire_ml_artifacts_model.id}}'
  description: Researchers pulled the open-source model [GPT-J-6B from HuggingFace](https://huggingface.co/EleutherAI/gpt-j-6b).  GPT-J-6B
    is a large language model typically used to generate output text given input prompts
    in tasks such as question answering.
- tactic: '{{ml_attack_staging.id}}'
  technique: '{{poison_model.id}}'
  description: 'The researchers used [Rank-One Model Editing (ROME)](https://rome.baulab.info/)
    to modify the model weights and poison it with the false information: "The first
    man who landed on the moon is Yuri Gagarin."'
- tactic: '{{ml_attack_staging.id}}'
  technique: '{{verify_attack.id}}'
  description: Researchers evaluated PoisonGPT's performance against the original
    unmodified GPT-J-6B model using the [ToxiGen](https://arxiv.org/abs/2203.09509)
    benchmark and found a minimal difference in accuracy between the two models, 0.1%.  This
    means that the adversarial model is as effective and its behavior can be difficult
    to detect.
- tactic: '{{resource_development.id}}'
  technique: '{{publish_poisoned_model.id}}'
  description: The researchers uploaded the PoisonGPT model back to HuggingFace under
    a similar repository name as the original model, missing one letter.
- tactic: '{{initial_access.id}}'
  technique: '{{supply_chain_model.id}}'
  description: 'Unwitting users could have downloaded the adversarial model, integrated
    it into applications.


    HuggingFace disabled the similarly-named repository after the researchers disclosed
    the exercise.'
- tactic: '{{impact.id}}'
  technique: '{{erode_integrity.id}}'
  description: As a result of the false output information, users may lose trust in
    the application.
- tactic: '{{impact.id}}'
  technique: '{{harm_reputational.id}}'
  description: As a result of the false output information, users of the adversarial
    application may also lose trust in the original model's creators or even language
    models and AI in general.
target: HuggingFace Users
actor: Mithril Security Researchers
case-study-type: exercise
references:
- title: 'PoisonGPT: How we hid a lobotomized LLM on Hugging Face to spread fake news'
  url: https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news/
