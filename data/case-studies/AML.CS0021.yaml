---
id: AML.CS0021
name: ChatGPT Conversation Exfiltration
object-type: case-study
summary: '[Embrace the Red](https://embracethered.com/blog/) demonstrated that ChatGPT
  users'' conversations can be exfiltrated via an indirect prompt injection. To execute
  the attack, a threat actor uploads a malicious prompt to a public website, where
  a ChatGPT user may interact with it. The prompt causes ChatGPT to respond with the
  markdown for an image, whose URL has the user''s conversation secretly embedded.
  ChatGPT renders the image for the user, creating a automatic request to an adversary-controlled
  script and exfiltrating the user''s conversation. Additionally, the researcher demonstrated
  how the prompt can execute other plugins, opening them up to additional harms.'
incident-date: 2023-05-01
incident-date-granularity: MONTH
procedure:
- tactic: '{{resource_development.id}}'
  technique: '{{llm_prompt_crafting.id}}'
  description: The researcher developed a prompt that causes ChatGPT to include a
    Markdown element for an image with the user's conversation embedded in the URL
    as part of its responses.
- tactic: '{{resource_development.id}}'
  technique: '{{stage_cap.id}}'
  description: The researcher included the prompt in a webpage, where it could be
    retrieved by ChatGPT.
- tactic: '{{initial_access.id}}'
  technique: '{{drive_by_compromise.id}}'
  description: When the user makes a query that causes ChatGPT to retrieve the webpage
    using its `WebPilot` plugin, it ingests the adversary's prompt.
- tactic: '{{execution.id}}'
  technique: '{{pi_indirect.id}}'
  description: The prompt injection is executed, causing ChatGPT to include a Markdown
    element for an image hosted on an adversary-controlled server and embed the user's
    chat history as query parameter in the URL.
- tactic: '{{exfiltration.id}}'
  technique: '{{llm_rendering.id}}'
  description: ChatGPT automatically renders the image for the user, making the request
    to the adversary's server for the image contents, and exfiltrating the user's
    conversation.
- tactic: '{{privilege_escalation.id}}'
  technique: '{{llm_plugin_compromise.id}}'
  description: Additionally, the prompt can cause the LLM to execute other plugins
    that do not match a user request. In this instance, the researcher demonstrated
    the `WebPilot` plugin making a call to the `Expedia` plugin.
- tactic: '{{impact.id}}'
  technique: '{{harm_user.id}}'
  description: The user's privacy is violated, and they are potentially open to further
    targeted attacks.
target: OpenAI ChatGPT
actor: Embrace The Red
case-study-type: exercise
references:
- title: 'ChatGPT Plugins: Data Exfiltration via Images & Cross Plugin Request Forgery'
  url: https://embracethered.com/blog/posts/2023/chatgpt-webpilot-data-exfil-via-markdown-injection/
