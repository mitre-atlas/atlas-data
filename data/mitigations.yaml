---
- &limit_info_release
  id: AML.M0000
  name: Limit Release of Public Information
  object-type: mitigation
  description: |
    Limit the public release of technical information about machine learning used in an organization's products or services. Technical knowledge of how machine learning is used can be leveraged by adversaries to perform targeting and tailor attacks to the target system. Additionally, consider limiting the release of organizational information - including physical locations, researcher names, and department structures - from which technical details such as machine learning techniques, model architectures, or datasets may be inferred.
  techniques:
    - id: '{{victim_research.id}}'
      use: |
        Limit the connection between publicly disclosed approaches and the data, models, and algorithms used in production.
    - id: '{{victim_website.id}}'
      use: |
        Restrict release of technical information on ML-enabled products and organizational information on the teams supporting ML-enabled products.
- &limit_model_release
  id: AML.M0001
  name: Limit Model Artifact Release
  object-type: mitigation
  description: |
    Limit public release of technical project details including data, algorithms, model architectures, and model checkpoints that are used in production, or that are representative of those used in production.
  techniques:
    - id: '{{acquire_ml_artifacts_data.id}}'
      use: |
        Limiting the release of datasets can reduce an adversary's ability to target production models trained on the same or similar data.
    - id: '{{acquire_ml_artifacts_model.id}}'
      use: |
        Limiting the release of model architectures and checkpoints can reduce an adversaryâ€™s ability to target those models.
    - id: '{{poison_data.id}}'
      use: |
        Published datasets can be a target for poisoning attacks.
- &passive_output_obfuscation
  id: AML.M0002
  name: Passive ML Output Obfuscation
  object-type: mitigation
  description: |
    Limit outputs available to end user which can be used to reconstruct the model.
  techniques:
    - id: '{{discover_model_ontology.id}}'
      use: |
        Restrict the number of results shown. Limit the number of classes granted API access. Use randomized smoothing techniques. Reduce the precision of numerical outputs.
    - id: '{{discover_model_family.id}}'
      use: |
        Suggested approaches:
          - Restrict the number of results shown
          - Limit the number of classes granted API access
          - Use randomized smoothing techniques
          - Reduce the precision of numerical outputs
    - id: '{{craft_adv_blackbox.id}}'
      use: |
        Suggested approaches:
          - Restrict the number of results shown
          - Limit the number of classes granted API access
          - Use randomized smoothing techniques
          - Reduce the precision of numerical outputs
- &model_hardening
  id: AML.M0003
  name: Model Hardening
  object-type: mitigation
  description: |
    Incorporate adversarial examples and adjust training algorithms to ensure that models are robust to adversarial inputs.
  techniques:
    - id: '{{discover_model_ontology.id}}'
      use: |
        Combine models from different families to prevent a single point of failure.
    - id: '{{discover_model_family.id}}'
      use: |
        Combine models from different families to prevent a single point of failure.
    - id: '{{evade_model.id}}'
      use: |
        Incorporate adversarial examples during model training.
    - id: '{{exfiltrate_via_api.id}}'
      use: |
        Incorporate adversarial examples during model training. Identify fake data during inference time.
    - id: '{{erode_integrity.id}}'
      use: |
        Incorporate adversarial examples during model training. Monitor evaluation metrics for drops in performance.
    - id: '{{chaff_data.id}}'
      use: |
        Incorporate adversarial examples during model training. Identify fake data during inference time.
- &restrict_queries
  id: AML.M0004
  name: Restrict Number of ML Model Queries
  object-type: mitigation
  description: |
    Limit the number and rate of queries a user can do in a given period of time and detecting usage per user by tracking network traffic.
  techniques:
    - id: '{{cost_harvesting.id}}'
      use: |
        Cap the number of queries users can perform in a given interval to hinder an attacker's ability to send computationally expensive inputs
    - id: '{{discover_model_ontology.id}}'
      use: |
        Limit the amount of information an attacker can learn about a model's ontology through API queries.
    - id: '{{discover_model_family.id}}'
      use: |
        Limit the amount of information an attacker can learn about a model's ontology through API queries.
    - id: '{{exfiltrate_via_api.id}}'
      use: |
        Restrict the volume of API queries in a given period of time to regulate the amount and fidelity of potentially sensitive information an attacker can learn.
    - id: '{{membership_inference.id}}'
      use: |
        Restrict the volume of API queries in a given period of time to regulate the amount and fidelity of potentially sensitive information an attacker can learn.
    - id: '{{model_inversion.id}}'
      use: |
        Restrict the volume of API queries in a given period of time to regulate the amount and fidelity of potentially sensitive information an attacker can learn.
    - id: '{{extract_model.id}}'
      use: |
        Restrict the volume of API queries in a given period of time to regulate the amount and fidelity of potentially sensitive information an attacker can learn.
    - id: '{{craft_adv_blackbox.id}}'
      use: |
        Cap the number of queries users can perform in a given interval to shrink the attack surface for black-box attacks.
- &control_access_rest
  id: AML.M0005
  name: Control Access to ML Models and Data at Rest
  object-type: mitigation
  description: |
    Establish access controls on internal model registries and limit internal access to production models. Limit access to training data only to approved users and encrypt data at rest.
  techniques:
    - id: '{{supply_chain_data.id}}'
      use: |
        Detect and remove bad data prior to use.
    - id: '{{poison_data.id}}'
      use: |
        Poisoned data can be detected and remediated before use.
    - id: '{{publish_poisoned_data.id}}'
      use: |
        Poisoned datasets can be detected and remediated before use.
    - id: '{{poison_model.id}}'
      use: |
        Prevent attackers from leveraging poisoned datasets to launch backdoor attacks against a model.
    - id: '{{inject_payload.id}}'
      use: |
        Prevent attackers from leveraging poisoned datasets to launch backdoor attacks against a model.
    - id: '{{supply_chain_model.id}}'
      use: |
        Ensure integrity of the model is preserved while in storage by preventing unwanted access.
- &ensemble_methods
  id: AML.M0006
  name: Use Ensemble Methods
  object-type: mitigation
  description: |
    Use a variety of different models to obfuscate where and how specific models are used and avoid a single point of failure. Some attacks may effectively evade one model or model family but be ineffective against others.
  techniques:
    - id: '{{craft_adv_blackbox.id}}'
      use: |
        Use multiple different models to fool adversaries of which type of model is used and how the model is used.
    - id: '{{supply_chain_software.id}}'
      use: |
        Using multiple different models ensures minimal performance loss if security flaw is found in tool for one model or family.
    - id: '{{supply_chain_model.id}}'
      use: |
        Using multiple different models ensures minimal performance loss if security flaw is found in tool for one model or family.
    - id: '{{full_access.id}}'
      use: |
        Use multiple different models to fool adversaries of which type of model is used and how the model used.
    - id: '{{discover_model_family.id}}'
      use: |
        Use multiple different models to fool adversaries of which type of model is used and how the model used.
- &sanitize_training_data
  id: AML.M0007
  name: Sanitize Training Data
  object-type: mitigation
  description: |
    Detect and remove or remediate poisoned training data.  Training data should be sanitized prior to model training and recurrently for an active learning model.

    Implement a filter to limit ingested training data.  Establish a content policy that would remove unwanted content such as certain explicit or offensive language from being used.
  techniques:
    - id: '{{supply_chain_data.id}}'
      use: |
        Detect and remove or remediate poisoned data to avoid adversarial model drift or backdoor attacks.
    - id: '{{poison_data.id}}'
      use: |
        Detect modification of data and labels which may cause adversarial model drift or backdoor attacks.
    - id: '{{poison_model.id}}'
      use: |
        Prevent attackers from leveraging poisoned datasets to launch backdoor attacks against a model.
- &validate_model
  id: AML.M0008
  name: Validate ML Model
  object-type: mitigation
  description: |
    Validate that machine learning models perform as intended by testing for backdoor triggers or adversarial bias.
  techniques:
    - id: '{{supply_chain_model.id}}'
      use: |
        Ensure that acquired models to not respond to potential backdoor triggers or adversarial bias.
    - id: '{{poison_model.id}}'
      use: |
        Ensure that trained models to not respond to potential backdoor triggers or adversarial bias.
    - id: '{{inject_payload.id}}'
      use: |
        Ensure that acquired models to not respond to potential backdoor triggers or adversarial bias.
- &multi_modal_sensors
  id: AML.M0009
  name: Use Multi-Modal Sensors
  object-type: mitigation
  description: |
    Incorporate multiple sensors to integrate varying perspectives and modalities to avoid a single point of failure susceptible to physical attacks.

    Face liveness detection in biometrics can be reinforced with methods such as stereo for depth sensing, pupil tracking, and texture and motion analysis.
  techniques:
    - id: '{{physical_env.id}}'
      use: |
        Using a variety of sensors can make it more difficult for an attacker with physical access to compromise and produce malicious results.
    - id: '{{evade_model.id}}'
      use: |
        Using a variety of sensors can make it more difficult for an attacker to compromise and produce malicious results.
- &input_restoration
  id: AML.M0010
  name: Input Restoration
  object-type: mitigation
  description: |
    Preprocess all inference data to nullify or reverse potential adversarial perturbations.
  techniques:
    - id: '{{craft_adv_blackbox.id}}'
      use: |
        Input restoration adds an extra layer of unknowns and randomness when an adversary evaluates the input-output relationship.
    - id: '{{evade_model.id}}'
      use: |
        Preprocessing model inputs can prevent malicious data from going through the machine learning pipeline.
    - id: '{{erode_integrity.id}}'
      use: |
        Preprocessing model inputs can prevent malicious data from going through the machine learning pipeline.
    - id: '{{chaff_data.id}}'
      use: |
        Preprocessing inference model inputs can prevent chaff data from going through the machien learning pipeline.
- &encrypt_info
  id: AML.M0012
  name: Encrypt Sensitive Information
  object-type: mitigation
  description: |
    Encrypt the ML model when packaging in applications to protect against adversaries attempting to access sensitive data.
  ATT&CK-reference:
    id: M1041
    url: https://attack.mitre.org/mitigations/M1041/
  techniques:
    - id: '{{search_apps.id}}'
      use: |
        Obfuscate model files via compression to prevent attackers from easily obtaining artifacts from repositories.
    - id: '{{active_scanning.id}}'
      use: |
        Protect the system's information from being accessed by an adversary.
    - id: '{{discover_ml_artifacts.id}}'
      use: |
        Protect machine learning artifacts from adversaries who gather private information to target and improve attacks.
- &code_signing
  id: AML.M0013
  name: Code Signing
  object-type: mitigation
  description: |
    Enforce binary and application integrity with digital signature verification to prevent untrusted code from executing. Adversaries can embed malicious code in ML software or models. Enforcement of code signing can prevent the compromise of the machine learning supply chain and prevent execution of malicious code.
  techniques:
    - id: '{{unsafe_ml_artifacts.id}}'
      use: |
        Prevent execution of ML artifacts that are not properly signed.
    - id: '{{supply_chain_software.id}}'
      use: |
        Enforce properly signed drivers and ML software frameworks.
    - id: '{{supply_chain_model.id}}'
      use: |
        Enforce properly signed model files.
  ATT&CK-reference:
    id: M1045
    url: https://attack.mitre.org/mitigations/M1045/
- &verify_ml_artifacts
  id: AML.M0014
  name: Verify ML Artifacts
  object-type: mitigation
  description: |
    Verify the cryptographic checksum of all machine learning artifacts to verify that the file was not modified by an attacker.
  techniques:
    - id: '{{publish_poisoned_data.id}}'
      use: |
        Determine validity of published data in order to avoid using poisoned data that introduces vulnerabilities.
    - id: '{{unsafe_ml_artifacts.id}}'
      use: |
        Introduce proper checking of signatures to ensure that unsafe ML artifacts will not be executed in the system.
    - id: '{{supply_chain.id}}'
      use: |
        Allow only certified users access to artifacts and prevent access to the supply chain from outside sources.
- &distribution_methods
  id: AML.M0017
  name: Model Distribution Methods
  object-type: mitigation
  description: |
    Deploying ML models to edge devices can increase the attack surface of the system. Consider serving models in the cloud to reduce the level of access the adversary has to the model.
  techniques:
    - id: '{{full_access.id}}'
      use: |
        Not distributing the model in software to edge devices, can limit an adversary's ability to gain full access to the model.
    - id: '{{craft_adv_whitebox.id}}'
      use: |
        With full access to the model, an adversary could perform white-box attacks.
    - id: '{{supply_chain_model.id}}'
      use: |
        An adversary could repackage the application with a malicious version of the model.
- &user_training
  id: AML.M0019
  name: User Training
  object-type: mitigation
  description: |
    Educate ML model developers on secure coding practices and ML vulnerabilities.
  techniques:
    - id: '{{user_execution.id}}'
      use: |
        Training users to be able to identify attempts at manipulation will make them less susceptible to performing techniques that cause the execution of malicious code.
    - id: '{{unsafe_ml_artifacts.id}}'
      use: |
        Train users to identify attempts of manipulation to prevent them from running unsafe code which when executed could develop unsafe artifacts. These artifacts may have a detrimental effect on the system.
- &vuln_scanning
  id: AML.M0020
  name: Vulnerability Scanning
  object-type: mitigation
  description: |
    Vulnerability scanning is used to find potentially exploitable software vulnerabilities to remediate them.

    File formats such as pickle files that are commonly used to store machine learning models can contain exploits that allow for arbitrary code execution.
  techniques:
    - id: '{{unsafe_ml_artifacts.id}}'
      use: |
        Scan ML artifacts for vulnerabilities before execution.
  ATT&CK-reference:
    id: M1016
    url: https://attack.mitre.org/mitigations/M1016/
- &restrict_lib_loading
  id: AML.M0021
  name: Restrict Library Loading
  object-type: mitigation
  description: |
    Prevent abuse of library loading mechanisms in the operating system and software to load untrusted code by configuring appropriate library loading mechanisms and investigating potential vulnerable software.

    File formats such as pickle files that are commonly used to store machine learning models can contain exploits that allow for loading of malicious libraries.
  techniques:
    - id: '{{unsafe_ml_artifacts.id}}'
      use: |
        Restrict library loading by ML artifacts.
  ATT&CK-reference:
    id: M1044
    url: https://attack.mitre.org/mitigations/M1044/
