---
- &limit_info_release
  id: AML.M0000
  name: Limit Release of Public Information
  object-type: mitigation
  category:
    - Policy
  ML-lifecycle:
    - Business and Data Understanding
  description: >
    Limit the public release of technical information about the machine learning stack used in an organization's products or services.
    Technical knowledge of how machine learning is used can be leveraged by adversaries to perform targeting and tailor attacks to the target system.
    Additionally, consider limiting the release of organizational information - including physical locations, researcher names, and department structures - from which technical details such as machine learning techniques, model architectures, or datasets may be inferred.
  techniques:
    - id: '{{victim_research.id}}'
      use: |
        Limit the connection between publicly disclosed approaches and the data, models, and algorithms used in production.
    - id: '{{victim_website.id}}'
      use: |
        Restrict release of technical information on ML-enabled products and organizational information on the teams supporting ML-enabled products.
- &limit_model_release
  id: AML.M0001
  name: Limit Model Artifact Release
  object-type: mitigation
  category:
    - Policy
  ML-lifecycle:
    - Business and Data Understanding
    - Deployment
  description: |
    Limit public release of technical project details including data, algorithms, model architectures, and model checkpoints that are used in production, or that are representative of those used in production.
  techniques:
    - id: '{{acquire_ml_artifacts_data.id}}'
      use: |
        Limiting the release of datasets can reduce an adversary's ability to target production models trained on the same or similar data.
    - id: '{{acquire_ml_artifacts_model.id}}'
      use: |
        Limiting the release of model architectures and checkpoints can reduce an adversary's ability to target those models.
    - id: '{{poison_data.id}}'
      use: |
        Published datasets can be a target for poisoning attacks.
- &passive_output_obfuscation
  id: AML.M0002
  name: Passive ML Output Obfuscation
  object-type: mitigation
  category:
    - Technical - ML
  ML-lifecycle:
    - ML Model Evaluation
    - Deployment
  description: |
    Decreasing the fidelity of model outputs provided to the end user can reduce an adversaries ability to extract information about the model and optimize attacks for the model.
  techniques:
    - id: '{{discover_model_ontology.id}}'
      use: |
        Suggested approaches:
          - Restrict the number of results shown
          - Limit specificity of output class ontology
          - Use randomized smoothing techniques
          - Reduce the precision of numerical outputs
    - id: '{{discover_model_family.id}}'
      use: |
        Suggested approaches:
          - Restrict the number of results shown
          - Limit specificity of output class ontology
          - Use randomized smoothing techniques
          - Reduce the precision of numerical outputs
    - id: '{{craft_adv_blackbox.id}}'
      use: |
        Suggested approaches:
          - Restrict the number of results shown
          - Limit specificity of output class ontology
          - Use randomized smoothing techniques
          - Reduce the precision of numerical outputs
    - id: '{{membership_inference.id}}'
      use: |
        Suggested approaches:
          - Restrict the number of results shown
          - Limit specificity of output class ontology
          - Use randomized smoothing techniques
          - Reduce the precision of numerical outputs
    - id: '{{model_inversion.id}}'
      use: |
        Suggested approaches:
          - Restrict the number of results shown
          - Limit specificity of output class ontology
          - Use randomized smoothing techniques
          - Reduce the precision of numerical outputs
    - id: '{{extract_model.id}}'
      use: |
        Suggested approaches:
          - Restrict the number of results shown
          - Limit specificity of output class ontology
          - Use randomized smoothing techniques
          - Reduce the precision of numerical outputs
- &model_hardening
  id: AML.M0003
  name: Model Hardening
  object-type: mitigation
  category:
    - Technical - ML
  ML-lifecycle:
    - Data Preparation
    - ML Model Engineering
  description: |
    Use techniques to make machine learning models robust to adversarial inputs such as adversarial training or network distillation.
  techniques:
    - id: '{{evade_model.id}}'
      use: |
        Hardened models are more difficult to evade.
    - id: '{{erode_integrity.id}}'
      use: |
        Hardened models are less susceptible to integrity attacks.
- &restrict_queries
  id: AML.M0004
  name: Restrict Number of ML Model Queries
  object-type: mitigation
  category:
    - Technical - Cyber
  ML-lifecycle:
    - Business and Data Understanding
    - Deployment
    - Monitoring and Maintenance
  description: |
    Limit the total number and rate of queries a user can perform.
  techniques:
    - id: '{{cost_harvesting.id}}'
      use: |
        Limit the number of queries users can perform in a given interval to hinder an attacker's ability to send computationally expensive inputs
    - id: '{{discover_model_ontology.id}}'
      use: |
        Limit the amount of information an attacker can learn about a model's ontology through API queries.
    - id: '{{discover_model_family.id}}'
      use: |
        Limit the amount of information an attacker can learn about a model's ontology through API queries.
    - id: '{{exfiltrate_via_api.id}}'
      use: |
        Limit the volume of API queries in a given period of time to regulate the amount and fidelity of potentially sensitive information an attacker can learn.
    - id: '{{membership_inference.id}}'
      use: |
        Limit the volume of API queries in a given period of time to regulate the amount and fidelity of potentially sensitive information an attacker can learn.
    - id: '{{model_inversion.id}}'
      use: |
        Limit the volume of API queries in a given period of time to regulate the amount and fidelity of potentially sensitive information an attacker can learn.
    - id: '{{extract_model.id}}'
      use: |
        Limit the volume of API queries in a given period of time to regulate the amount and fidelity of potentially sensitive information an attacker can learn.
    - id: '{{craft_adv_blackbox.id}}'
      use: |
        Limit the number of queries users can perform in a given interval to shrink the attack surface for black-box attacks.
    - id: '{{ml_dos.id}}'
      use: |
        Limit the number of queries users can perform in a given interval to prevent a denial of service.
    - id: '{{chaff_data.id}}'
      use: |
        Limit the number of queries users can perform in a given interval to protect the system from chaff data spam.
- &control_access_rest
  id: AML.M0005
  name: Control Access to ML Models and Data at Rest
  object-type: mitigation
  category:
    - Policy
  ML-lifecycle:
    - Business and Data Understanding
    - Data Preparation
    - ML Model Engineering
    - ML Model Evaluation
  description: |
    Establish access controls on internal model registries and limit internal access to production models. Limit access to training data only to approved users.
  techniques:
    - id: '{{supply_chain_data.id}}'
      use: |
        Access controls can prevent tampering with ML artifacts and prevent unauthorized copying.
    - id: '{{poison_data.id}}'
      use: |
        Access controls can prevent tampering with ML artifacts and prevent unauthorized copying.
    - id: '{{poison_model.id}}'
      use: |
        Access controls can prevent tampering with ML artifacts and prevent unauthorized copying.
    - id: '{{inject_payload.id}}'
      use: |
        Access controls can prevent tampering with ML artifacts and prevent unauthorized copying.
    - id: '{{supply_chain_model.id}}'
      use: |
        Access controls can prevent tampering with ML artifacts and prevent unauthorized copying.
    - id: '{{exfiltrate_via_cyber.id}}'
      use: |
        Access controls can prevent exfiltration.
    - id: '{{ip_theft.id}}'
      use: |
        Access controls can prevent theft of intellectual property.
- &ensemble_methods
  id: AML.M0006
  name: Use Ensemble Methods
  object-type: mitigation
  category:
    - Technical - ML
  ML-lifecycle:
    - ML Model Engineering
  description: |
    Use an ensemble of models for inference to increase robustness to adversarial inputs. Some attacks may effectively evade one model or model family but be ineffective against others.
  techniques:
    - id: '{{erode_integrity.id}}'
      use: |
        Using multiple different models increases robustness to attack.
    - id: '{{supply_chain_software.id}}'
      use: |
        Using multiple different models ensures minimal performance loss if security flaw is found in tool for one model or family.
    - id: '{{supply_chain_model.id}}'
      use: |
        Using multiple different models ensures minimal performance loss if security flaw is found in tool for one model or family.
    - id: '{{evade_model.id}}'
      use: |
        Using multiple different models increases robustness to attack.
    - id: '{{discover_model_family.id}}'
      use: |
        Use multiple different models to fool adversaries of which type of model is used and how the model used.
- &sanitize_training_data
  id: AML.M0007
  name: Sanitize Training Data
  object-type: mitigation
  category:
    - Technical - ML
  ML-lifecycle:
    - Business and Data Understanding
    - Data Preparation
    - Monitoring and Maintenance
  description: |
    Detect and remove or remediate poisoned training data.  Training data should be sanitized prior to model training and recurrently for an active learning model.

    Implement a filter to limit ingested training data.  Establish a content policy that would remove unwanted content such as certain explicit or offensive language from being used.
  techniques:
    - id: '{{supply_chain_data.id}}'
      use: |
        Detect and remove or remediate poisoned data to avoid adversarial model drift or backdoor attacks.
    - id: '{{poison_data.id}}'
      use: |
        Detect modification of data and labels which may cause adversarial model drift or backdoor attacks.
    - id: '{{poison_model.id}}'
      use: |
        Prevent attackers from leveraging poisoned datasets to launch backdoor attacks against a model.
- &validate_model
  id: AML.M0008
  name: Validate ML Model
  object-type: mitigation
  category:
    - Technical - ML
  ML-lifecycle:
    - ML Model Evaluation
    - Monitoring and Maintenance
  description: |
    Validate that machine learning models perform as intended by testing for backdoor triggers or adversarial bias.
  techniques:
    - id: '{{supply_chain_model.id}}'
      use: |
        Ensure that acquired models do not respond to potential backdoor triggers or adversarial bias.
    - id: '{{poison_model.id}}'
      use: |
        Ensure that trained models do not respond to potential backdoor triggers or adversarial bias.
    - id: '{{inject_payload.id}}'
      use: |
        Ensure that acquired models do not respond to potential backdoor triggers or adversarial bias.
- &multi_modal_sensors
  id: AML.M0009
  name: Use Multi-Modal Sensors
  object-type: mitigation
  category:
    - Technical - Cyber
  ML-lifecycle:
    - Business and Data Understanding
    - Data Preparation
    - ML Model Engineering
  description: |
    Incorporate multiple sensors to integrate varying perspectives and modalities to avoid a single point of failure susceptible to physical attacks.
  techniques:
    - id: '{{physical_env.id}}'
      use: |
        Using a variety of sensors can make it more difficult for an attacker with physical access to compromise and produce malicious results.
    - id: '{{evade_model.id}}'
      use: |
        Using a variety of sensors can make it more difficult for an attacker to compromise and produce malicious results.
- &input_restoration
  id: AML.M0010
  name: Input Restoration
  object-type: mitigation
  category:
    - Technical - ML
  ML-lifecycle:
    - Data Preparation
    - ML Model Evaluation
    - Deployment
    - Monitoring and Maintenance
  description: |
    Preprocess all inference data to nullify or reverse potential adversarial perturbations.
  techniques:
    - id: '{{craft_adv_blackbox.id}}'
      use: |
        Input restoration adds an extra layer of unknowns and randomness when an adversary evaluates the input-output relationship.
    - id: '{{evade_model.id}}'
      use: |
        Preprocessing model inputs can prevent malicious data from going through the machine learning pipeline.
    - id: '{{erode_integrity.id}}'
      use: |
        Preprocessing model inputs can prevent malicious data from going through the machine learning pipeline.
- &restrict_lib_loading
  id: AML.M0011
  name: Restrict Library Loading
  object-type: mitigation
  category:
    - Technical - Cyber
  ML-lifecycle:
    - Deployment
  description: |
    Prevent abuse of library loading mechanisms in the operating system and software to load untrusted code by configuring appropriate library loading mechanisms and investigating potential vulnerable software.

    File formats such as pickle files that are commonly used to store machine learning models can contain exploits that allow for loading of malicious libraries.
  techniques:
    - id: '{{unsafe_ml_artifacts.id}}'
      use: |
        Restrict library loading by ML artifacts.
  ATT&CK-reference:
    id: M1044
    url: https://attack.mitre.org/mitigations/M1044/
- &encrypt_info
  id: AML.M0012
  name: Encrypt Sensitive Information
  object-type: mitigation
  category:
    - Technical - Cyber
  ML-lifecycle:
    - Deployment
  description: |
    Encrypt sensitive data such as ML models to protect against adversaries attempting to access sensitive data.
  ATT&CK-reference:
    id: M1041
    url: https://attack.mitre.org/mitigations/M1041/
  techniques:
    - id: '{{ml_artifact_collection.id}}'
      use: |
        Protect machine learning artifacts with encryption.
    - id: '{{ip_theft.id}}'
      use: |
        Protect machine learning artifacts with encryption.
    - id: '{{discover_ml_artifacts.id}}'
      use: |
        Protect machine learning artifacts from adversaries who gather private information to target and improve attacks.
- &code_signing
  id: AML.M0013
  name: Code Signing
  object-type: mitigation
  category:
    - Technical - Cyber
  ML-lifecycle:
    - Deployment
  description: |
    Enforce binary and application integrity with digital signature verification to prevent untrusted code from executing. Adversaries can embed malicious code in ML software or models. Enforcement of code signing can prevent the compromise of the machine learning supply chain and prevent execution of malicious code.
  techniques:
    - id: '{{unsafe_ml_artifacts.id}}'
      use: |
        Prevent execution of ML artifacts that are not properly signed.
    - id: '{{supply_chain_software.id}}'
      use: |
        Enforce properly signed drivers and ML software frameworks.
    - id: '{{supply_chain_model.id}}'
      use: |
        Enforce properly signed model files.
  ATT&CK-reference:
    id: M1045
    url: https://attack.mitre.org/mitigations/M1045/
- &verify_ml_artifacts
  id: AML.M0014
  name: Verify ML Artifacts
  object-type: mitigation
  category:
    - Technical - Cyber
  ML-lifecycle:
    - Business and Data Understanding
    - Data Preparation
    - ML Model Engineering
  description: |
    Verify the cryptographic checksum of all machine learning artifacts to verify that the file was not modified by an attacker.
  techniques:
    - id: '{{publish_poisoned_data.id}}'
      use: |
        Determine validity of published data in order to avoid using poisoned data that introduces vulnerabilities.
    - id: '{{unsafe_ml_artifacts.id}}'
      use: |
        Introduce proper checking of signatures to ensure that unsafe ML artifacts will not be executed in the system.
    - id: '{{supply_chain.id}}'
      use: |
        Introduce proper checking of signatures to ensure that unsafe ML artifacts will not be introduced to the system.
- &adv_input_detection
  id: AML.M0015
  name: Adversarial Input Detection
  object-type: mitigation
  category:
    - Technical - ML
  ML-lifecycle:
    - Data Preparation
    - ML Model Engineering
    - ML Model Evaluation
    - Deployment
    - Monitoring and Maintenance
  description: >
    Detect and block adversarial inputs or atypical queries that deviate from known benign behavior, exhibit behavior patterns observed in previous attacks or that come from potentially malicious IPs.

    Incorporate adversarial detection algorithms into the ML system prior to the ML model.
  techniques:
    - id: '{{evade_model.id}}'
      use: |
        Prevent an attacker from introducing adversarial data into the system.
    - id: '{{craft_adv_blackbox.id}}'
      use: |
        Monitor queries and query patterns to the target model, block access if suspicious queries are detected.
    - id: '{{ml_dos.id}}'
      use: |
        Assess queries before inference call or enforce timeout policy for queries which consume excessive resources.
    - id: '{{erode_integrity.id}}'
      use: |
        Incorporate adversarial input detection into the pipeline before inputs reach the model.
- &vuln_scanning
  id: AML.M0016
  name: Vulnerability Scanning
  object-type: mitigation
  category:
    - Technical - Cyber
  ML-lifecycle:
    - ML Model Engineering
    - Deployment
    - Monitoring and Maintenance
  description: |
    Vulnerability scanning is used to find potentially exploitable software vulnerabilities to remediate them.

    File formats such as pickle files that are commonly used to store machine learning models can contain exploits that allow for arbitrary code execution.
  techniques:
    - id: '{{unsafe_ml_artifacts.id}}'
      use: |
        Scan ML artifacts for vulnerabilities before execution.
  ATT&CK-reference:
    id: M1016
    url: https://attack.mitre.org/mitigations/M1016/
- &distribution_methods
  id: AML.M0017
  name: Model Distribution Methods
  object-type: mitigation
  category:
    - Policy
  ML-lifecycle:
    - Deployment
  description: |
    Deploying ML models to edge devices can increase the attack surface of the system. Consider serving models in the cloud to reduce the level of access the adversary has to the model.
  techniques:
    - id: '{{full_access.id}}'
      use: |
        Not distributing the model in software to edge devices, can limit an adversary's ability to gain full access to the model.
    - id: '{{craft_adv_whitebox.id}}'
      use: |
        With full access to the model, an adversary could perform white-box attacks.
    - id: '{{supply_chain_model.id}}'
      use: |
        An adversary could repackage the application with a malicious version of the model.
- &user_training
  id: AML.M0018
  name: User Training
  object-type: mitigation
  category:
    - Policy
  ML-lifecycle:
    - Business and Data Understanding
    - Data Preparation
    - ML Model Engineering
    - ML Model Evaluation
    - Deployment
    - Monitoring and Maintenance
  description: |
    Educate ML model developers on secure coding practices and ML vulnerabilities.
  techniques:
    - id: '{{user_execution.id}}'
      use: |
        Training users to be able to identify attempts at manipulation will make them less susceptible to performing techniques that cause the execution of malicious code.
    - id: '{{unsafe_ml_artifacts.id}}'
      use: |
        Train users to identify attempts of manipulation to prevent them from running unsafe code which when executed could develop unsafe artifacts. These artifacts may have a detrimental effect on the system.
  ATT&CK-reference:
    id: M1017
    url: https://attack.mitre.org/mitigations/M1017/
