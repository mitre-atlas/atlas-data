---

- &limit_info_release
  id: AML.M0000
  name: Limit Public Release of Information
  description: Limit the public release of technical information about the AI stack
    used in an organization's products or services. Technical knowledge of how AI
    is used can be leveraged by adversaries to perform targeting and tailor attacks
    to the target system. Additionally, consider limiting the release of organizational
    information - including physical locations, researcher names, and department structures
    - from which technical details such as AI techniques, model architectures, or
    datasets may be inferred.
  object-type: mitigation
  techniques:
  - id: '{{victim_research.id}}'
    use: 'Limit the connection between publicly disclosed approaches and the data,
      models, and algorithms used in production.

      '
  - id: '{{victim_website.id}}'
    use: 'Restrict release of technical information on ML-enabled products and organizational
      information on the teams supporting ML-enabled products.

      '
  - id: '{{acquire_ml_artifacts.id}}'
    use: 'Limit the release of sensitive information in the metadata of deployed systems
      and publicly available applications.

      '
  - id: '{{search_apps.id}}'
    use: 'Limit the release of sensitive information in the metadata of deployed systems
      and publicly available applications.

      '
  - id: '{{train_proxy_model.id}}'
    use: Limiting release of technical information about a model and training data
      can reduce an adversary's ability to create an accurate proxy model.
  - id: '{{proxy_via_artifacts.id}}'
    use: Limiting release of technical information about a model and training data
      can reduce an adversary's ability to create an accurate proxy model.
  - id: '{{pretrained_proxy.id}}'
    use: Limiting release of technical information about a model and training data
      can reduce an adversary's ability to create an accurate proxy model.
  ml-lifecycle:
  - Business and Data Understanding
  category:
  - Policy
  created_date: 2023-04-12
  modified_date: 2025-12-23

- &limit_model_release
  id: AML.M0001
  name: Limit Model Artifact Release
  description: 'Limit public release of technical project details including data,
    algorithms, model architectures, and model checkpoints that are used in production,
    or that are representative of those used in production.

    '
  object-type: mitigation
  techniques:
  - id: '{{acquire_ml_artifacts_data.id}}'
    use: 'Limiting the release of datasets can reduce an adversary''s ability to target
      production models trained on the same or similar data.

      '
  - id: '{{acquire_ml_artifacts_model.id}}'
    use: 'Limiting the release of model architectures and checkpoints can reduce an
      adversary''s ability to target those models.

      '
  - id: '{{poison_data.id}}'
    use: 'Published datasets can be a target for poisoning attacks.

      '
  - id: '{{train_proxy_model.id}}'
    use: Limiting the release of model artifacts can reduce an adversary's ability
      to create an accurate proxy model.
  - id: '{{ml_artifact_collection.id}}'
    use: Limiting the release of artifacts can reduce an adversary's ability to collect
      model artifacts
  - id: '{{proxy_via_artifacts.id}}'
    use: Limiting the release of model artifacts can reduce an adversary's ability
      to create an accurate proxy model.
  ml-lifecycle:
  - Business and Data Understanding
  - Deployment
  category:
  - Policy
  created_date: 2023-04-12
  modified_date: 2025-12-23

- &passive_output_obfuscation
  id: AML.M0002
  name: Passive AI Output Obfuscation
  description: Decreasing the fidelity of model outputs provided to the end user can
    reduce an adversary's ability to extract information about the model and optimize
    attacks for the model.
  object-type: mitigation
  techniques:
  - id: '{{discover_model_ontology.id}}'
    use: "Suggested approaches:\n  - Restrict the number of results shown\n  - Limit\
      \ specificity of output class ontology\n  - Use randomized smoothing techniques\n\
      \  - Reduce the precision of numerical outputs\n"
  - id: '{{discover_model_family.id}}'
    use: "Suggested approaches:\n  - Restrict the number of results shown\n  - Limit\
      \ specificity of output class ontology\n  - Use randomized smoothing techniques\n\
      \  - Reduce the precision of numerical outputs\n"
  - id: '{{craft_adv_blackbox.id}}'
    use: "Suggested approaches:\n  - Restrict the number of results shown\n  - Limit\
      \ specificity of output class ontology\n  - Use randomized smoothing techniques\n\
      \  - Reduce the precision of numerical outputs\n"
  - id: '{{membership_inference.id}}'
    use: "Suggested approaches:\n  - Restrict the number of results shown\n  - Limit\
      \ specificity of output class ontology\n  - Use randomized smoothing techniques\n\
      \  - Reduce the precision of numerical outputs\n"
  - id: '{{model_inversion.id}}'
    use: "Suggested approaches:\n  - Restrict the number of results shown\n  - Limit\
      \ specificity of output class ontology\n  - Use randomized smoothing techniques\n\
      \  - Reduce the precision of numerical outputs\n"
  - id: '{{extract_model.id}}'
    use: "Suggested approaches:\n  - Restrict the number of results shown\n  - Limit\
      \ specificity of output class ontology\n  - Use randomized smoothing techniques\n\
      \  - Reduce the precision of numerical outputs\n"
  - id: '{{craft_adv.id}}'
    use: Obfuscating model outputs reduces an adversary's ability to generate effective
      adversarial data.
  - id: '{{craft_adv_blackbox.id}}'
    use: Obfuscating model outputs reduces an adversary's ability to create effective
      adversarial inputs.
  - id: '{{train_proxy_model.id}}'
    use: Obfuscating model outputs can reduce an adversary's ability to produce an
      accurate proxy model.
  - id: '{{verify_attack.id}}'
    use: Obfuscating model outputs reduces an adversary's ability to verify the efficacy
      of an attack.
  - id: '{{replicate_model.id}}'
    use: Obfuscating model outputs restricts an adversary's ability to create an accurate
      proxy model by querying a model and observing its outputs.
  - id: '{{discover_model_outputs.id}}'
    use: Obfuscating model outputs can prevent adversaries from collecting sensitive
      information about the model outputs.
  ml-lifecycle:
  - Deployment
  - ML Model Evaluation
  category:
  - Technical - ML
  created_date: 2023-04-12
  modified_date: 2025-12-23

- &model_hardening
  id: AML.M0003
  name: Model Hardening
  description: Use techniques to make AI models robust to adversarial inputs such
    as adversarial training or network distillation.
  object-type: mitigation
  techniques:
  - id: '{{evade_model.id}}'
    use: 'Hardened models are more difficult to evade.

      '
  - id: '{{erode_integrity.id}}'
    use: 'Hardened models are less susceptible to integrity attacks.

      '
  - id: '{{craft_adv.id}}'
    use: Hardened models are more robust to adversarial inputs.
  - id: '{{craft_adv_blackbox.id}}'
    use: Hardened models are more robust to adversarial inputs.
  - id: '{{craft_adv_transfer.id}}'
    use: Hardened models are more robust to adversarial inputs.
  - id: '{{craft_adv_manual.id}}'
    use: Hardened models are more robust to adversarial inputs.
  - id: '{{craft_adv_whitebox.id}}'
    use: Hardened models are more robust to adversarial inputs.
  - id: '{{craft_adv_trigger.id}}'
    use: Hardened models are more robust to adversarial inputs.
  ml-lifecycle:
  - Data Preparation
  - ML Model Engineering
  category:
  - Technical - ML
  created_date: 2023-04-12
  modified_date: 2025-12-23

- &restrict_queries
  id: AML.M0004
  name: Restrict Number of AI Model Queries
  description: 'Limit the total number and rate of queries a user can perform.

    '
  object-type: mitigation
  techniques:
  - id: '{{cost_harvesting.id}}'
    use: 'Limit the number of queries users can perform in a given interval to hinder
      an attacker''s ability to send computationally expensive inputs

      '
  - id: '{{discover_model_ontology.id}}'
    use: 'Limit the amount of information an attacker can learn about a model''s ontology
      through API queries.

      '
  - id: '{{discover_model_family.id}}'
    use: 'Limit the amount of information an attacker can learn about a model''s ontology
      through API queries.

      '
  - id: '{{exfiltrate_via_api.id}}'
    use: 'Limit the volume of API queries in a given period of time to regulate the
      amount and fidelity of potentially sensitive information an attacker can learn.

      '
  - id: '{{membership_inference.id}}'
    use: 'Limit the volume of API queries in a given period of time to regulate the
      amount and fidelity of potentially sensitive information an attacker can learn.

      '
  - id: '{{model_inversion.id}}'
    use: 'Limit the volume of API queries in a given period of time to regulate the
      amount and fidelity of potentially sensitive information an attacker can learn.

      '
  - id: '{{extract_model.id}}'
    use: 'Limit the volume of API queries in a given period of time to regulate the
      amount and fidelity of potentially sensitive information an attacker can learn.

      '
  - id: '{{craft_adv_blackbox.id}}'
    use: 'Limit the number of queries users can perform in a given interval to shrink
      the attack surface for black-box attacks.

      '
  - id: '{{ml_dos.id}}'
    use: 'Limit the number of queries users can perform in a given interval to prevent
      a denial of service.

      '
  - id: '{{chaff_data.id}}'
    use: 'Limit the number of queries users can perform in a given interval to protect
      the system from chaff data spam.

      '
  - id: '{{craft_adv.id}}'
    use: Restricting the number of model queries can reduce an adversary's ability
      to refine and evaluate adversarial queries.
  - id: '{{craft_adv_blackbox.id}}'
    use: Restricting the number of queries to the model limits or slows an adversary's
      ability to perform black-box optimization attacks.
  - id: '{{craft_adv_manual.id}}'
    use: Restricting the number of model queries can reduce an adversary's ability
      to refine manually crafted adversarial inputs.
  - id: '{{train_proxy_model.id}}'
    use: Restricting the number of queries to the model decreases an adversary's ability
      to replicate an accurate proxy model.
  - id: '{{replicate_model.id}}'
    use: Restricting the number of queries to the model decreases an adversary's ability
      to replicate an accurate proxy model.
  - id: '{{verify_attack.id}}'
    use: Restricting the number of queries to the model decreases an adversary's ability
      to verify the efficacy of an attack.
  - id: '{{discover_llm_hallucinations.id}}'
    use: Restricting number of model queries limits or slows an adversary's ability
      to identify possible hallucinations.
  ml-lifecycle:
  - Business and Data Understanding
  - Deployment
  - Monitoring and Maintenance
  category:
  - Technical - Cyber
  created_date: 2023-04-12
  modified_date: 2025-12-23

- &control_access_rest
  id: AML.M0005
  name: Control Access to AI Models and Data at Rest
  description: 'Establish access controls on internal model registries and limit internal
    access to production models. Limit access to training data only to approved users.

    '
  object-type: mitigation
  techniques:
  - id: '{{supply_chain_data.id}}'
    use: 'Access controls can prevent tampering with ML artifacts and prevent unauthorized
      copying.

      '
  - id: '{{poison_data.id}}'
    use: 'Access controls can prevent tampering with ML artifacts and prevent unauthorized
      copying.

      '
  - id: '{{poison_model.id}}'
    use: 'Access controls can prevent tampering with ML artifacts and prevent unauthorized
      copying.

      '
  - id: '{{inject_payload.id}}'
    use: 'Access controls can prevent tampering with ML artifacts and prevent unauthorized
      copying.

      '
  - id: '{{supply_chain_model.id}}'
    use: 'Access controls can prevent tampering with ML artifacts and prevent unauthorized
      copying.

      '
  - id: '{{exfiltrate_via_cyber.id}}'
    use: 'Access controls can prevent exfiltration.

      '
  - id: '{{ip_theft.id}}'
    use: 'Access controls can prevent theft of intellectual property.

      '
  - id: '{{backdoor_model.id}}'
    use: Access controls can prevent tampering with AI artifacts and prevent unauthorized
      modification.
  - id: '{{craft_adv_whitebox.id}}'
    use: Access controls can reduce unnecessary access to AI models and prevent an
      adversary from achieving white-box access.
  - id: '{{discover_ml_artifacts.id}}'
    use: Access controls can limit an adversary's ability to identify AI models, datasets,
      and other artifacts on a system.
  - id: '{{full_access.id}}'
    use: Access controls on models and data at rest can help prevent full model access.
  - id: '{{ml_artifact_collection.id}}'
    use: Access controls can prevent or limit the collection of AI artifacts on the
      victim system.
  - id: '{{verify_attack.id}}'
    use: Access controls on models at rest can prevent an adversary's ability to verify
      attack efficacy.
  ml-lifecycle:
  - Business and Data Understanding
  - Data Preparation
  - ML Model Evaluation
  - ML Model Engineering
  category:
  - Policy
  created_date: 2023-04-12
  modified_date: 2025-12-23

- &ensemble_methods
  id: AML.M0006
  name: Use Ensemble Methods
  description: 'Use an ensemble of models for inference to increase robustness to
    adversarial inputs. Some attacks may effectively evade one model or model family
    but be ineffective against others.

    '
  object-type: mitigation
  techniques:
  - id: '{{erode_integrity.id}}'
    use: 'Using multiple different models increases robustness to attack.

      '
  - id: '{{supply_chain_software.id}}'
    use: 'Using multiple different models ensures minimal performance loss if security
      flaw is found in tool for one model or family.

      '
  - id: '{{supply_chain_model.id}}'
    use: 'Using multiple different models ensures minimal performance loss if security
      flaw is found in tool for one model or family.

      '
  - id: '{{evade_model.id}}'
    use: 'Using multiple different models increases robustness to attack.

      '
  - id: '{{discover_model_family.id}}'
    use: 'Use multiple different models to fool adversaries of which type of model
      is used and how the model used.

      '
  - id: '{{craft_adv_whitebox.id}}'
    use: Using an ensemble of models increases the difficulty of crafting effective
      adversarial data and improves overall robustness.
  - id: '{{craft_adv_blackbox.id}}'
    use: Using an ensemble of models increases the difficulty of crafting effective
      adversarial data and improves overall robustness.
  - id: '{{craft_adv_transfer.id}}'
    use: Using an ensemble of models increases the difficulty of crafting effective
      adversarial data and improves overall robustness.
  - id: '{{craft_adv_trigger.id}}'
    use: Using an ensemble of models increases the difficulty of crafting effective
      adversarial data and improves overall robustness.
  - id: '{{craft_adv_manual.id}}'
    use: Using an ensemble of models increases the difficulty of crafting effective
      adversarial data and improves overall robustness.
  - id: '{{craft_adv.id}}'
    use: Using an ensemble of models increases the difficulty of crafting effective
      adversarial data and improves overall robustness.
  ml-lifecycle:
  - ML Model Engineering
  category:
  - Technical - ML
  created_date: 2023-04-12
  modified_date: 2025-12-23

- &sanitize_training_data
  id: AML.M0007
  name: Sanitize Training Data
  description: 'Detect and remove or remediate poisoned training data.  Training data
    should be sanitized prior to model training and recurrently for an active learning
    model.


    Implement a filter to limit ingested training data.  Establish a content policy
    that would remove unwanted content such as certain explicit or offensive language
    from being used.

    '
  object-type: mitigation
  techniques:
  - id: '{{supply_chain_data.id}}'
    use: 'Detect and remove or remediate poisoned data to avoid adversarial model
      drift or backdoor attacks.

      '
  - id: '{{poison_data.id}}'
    use: 'Detect modification of data and labels which may cause adversarial model
      drift or backdoor attacks.

      '
  - id: '{{poison_model.id}}'
    use: 'Prevent attackers from leveraging poisoned datasets to launch backdoor attacks
      against a model.

      '
  - id: '{{erode_integrity_dataset.id}}'
    use: Remediating poisoned data can re-establish dataset integrity.
  ml-lifecycle:
  - Business and Data Understanding
  - Data Preparation
  - Monitoring and Maintenance
  category:
  - Technical - ML
  created_date: 2023-04-12
  modified_date: 2025-12-23

- &validate_model
  id: AML.M0008
  name: Validate AI Model
  description: 'Validate that AI models perform as intended by testing for backdoor
    triggers, potential for data leakage, or adversarial influence.

    Monitor AI model for concept drift and training data drift, which may indicate
    data tampering and poisoning.'
  object-type: mitigation
  techniques:
  - id: '{{supply_chain_model.id}}'
    use: Ensure that acquired models do not respond to potential backdoor triggers
      or adversarial influence.
  - id: '{{poison_model.id}}'
    use: Ensure that trained models do not respond to potential backdoor triggers
      or adversarial influence.
  - id: '{{inject_payload.id}}'
    use: Ensure that acquired models do not respond to potential backdoor triggers
      or adversarial influence.
  - id: '{{backdoor_model.id}}'
    use: Validating an AI model against a wide range of adversarial inputs can help
      increase confidence that the model has not been manipulated.
  - id: '{{craft_adv_trigger.id}}'
    use: Validating that an AI model does not respond to backdoor triggers can help
      increase confidence that the model has not been poisoned.
  - id: '{{poison_data.id}}'
    use: Robust evaluation of an AI model can help increase confidence that the model
      has not been poisoned.
  - id: '{{llm_data_leakage.id}}'
    use: Robust evaluation of an AI model can be used to detect privacy concerns,
      data leakage, and potential for revealing sensitive information.
  - id: '{{craft_adv.id}}'
    use: Validating an AI model against adversarial data can ensure the model is performing
      as intended and is robust to adversarial inputs.
  ml-lifecycle:
  - ML Model Evaluation
  - Monitoring and Maintenance
  category:
  - Technical - ML
  created_date: 2023-04-12
  modified_date: 2025-12-23

- &multi_modal_sensors
  id: AML.M0009
  name: Use Multi-Modal Sensors
  description: 'Incorporate multiple sensors to integrate varying perspectives and
    modalities to avoid a single point of failure susceptible to physical attacks.

    '
  object-type: mitigation
  techniques:
  - id: '{{physical_env.id}}'
    use: 'Using a variety of sensors can make it more difficult for an attacker with
      physical access to compromise and produce malicious results.

      '
  - id: '{{evade_model.id}}'
    use: 'Using a variety of sensors can make it more difficult for an attacker to
      compromise and produce malicious results.

      '
  - id: '{{gen_deepfake.id}}'
    use: Using a variety of sensors, such as IR depth cameras, can aid in detecting
      deepfakes.
  ml-lifecycle:
  - Business and Data Understanding
  - Data Preparation
  - ML Model Engineering
  category:
  - Technical - Cyber
  created_date: 2023-04-12
  modified_date: 2025-12-23

- &input_restoration
  id: AML.M0010
  name: Input Restoration
  description: 'Preprocess all inference data to nullify or reverse potential adversarial
    perturbations.

    '
  object-type: mitigation
  techniques:
  - id: '{{craft_adv_blackbox.id}}'
    use: 'Input restoration adds an extra layer of unknowns and randomness when an
      adversary evaluates the input-output relationship.

      '
  - id: '{{evade_model.id}}'
    use: 'Preprocessing model inputs can prevent malicious data from going through
      the machine learning pipeline.

      '
  - id: '{{erode_integrity.id}}'
    use: 'Preprocessing model inputs can prevent malicious data from going through
      the machine learning pipeline.

      '
  - id: '{{craft_adv.id}}'
    use: Input restoration can help remediate adversarial inputs.
  - id: '{{craft_adv_transfer.id}}'
    use: Input restoration can help remediate adversarial inputs.
  - id: '{{craft_adv_trigger.id}}'
    use: Input restoration can help remediate adversarial inputs.
  - id: '{{craft_adv_whitebox.id}}'
    use: Input restoration can help remediate adversarial inputs.
  - id: '{{craft_adv_manual.id}}'
    use: Input restoration can help remediate adversarial inputs.
  ml-lifecycle:
  - Data Preparation
  - ML Model Evaluation
  - Deployment
  - Monitoring and Maintenance
  category:
  - Technical - ML
  created_date: 2023-04-12
  modified_date: 2025-12-23

- &restrict_lib_loading
  id: AML.M0011
  name: Restrict Library Loading
  description: 'Prevent abuse of library loading mechanisms in the operating system
    and software to load untrusted code by configuring appropriate library loading
    mechanisms and investigating potential vulnerable software.


    File formats such as pickle files that are commonly used to store AI models can
    contain exploits that allow for loading of malicious libraries.'
  object-type: mitigation
  ATT&CK-reference:
    id: M1044
    url: https://attack.mitre.org/mitigations/M1044/
  techniques:
  - id: '{{unsafe_ml_artifacts.id}}'
    use: 'Restrict library loading by ML artifacts.

      '
  - id: '{{malicious_package.id}}'
    use: Restricting packages from loading external libraries can limit their ability
      to execute malicious code.
  - id: '{{user_execution.id}}'
    use: Restricting binaries from loading external libraries can limit their ability
      to execute malicious code.
  ml-lifecycle:
  - Deployment
  category:
  - Technical - Cyber
  created_date: 2023-04-12
  modified_date: 2025-12-23

- &encrypt_info
  id: AML.M0012
  name: Encrypt Sensitive Information
  description: Encrypt sensitive data such as AI models to protect against adversaries
    attempting to access sensitive data.
  object-type: mitigation
  ATT&CK-reference:
    id: M1041
    url: https://attack.mitre.org/mitigations/M1041/
  techniques:
  - id: '{{ml_artifact_collection.id}}'
    use: 'Protect machine learning artifacts with encryption.

      '
  - id: '{{ip_theft.id}}'
    use: 'Protect machine learning artifacts with encryption.

      '
  - id: '{{discover_ml_artifacts.id}}'
    use: Encrypting AI artifacts can protect against adversary attempts to discover
      sensitive information.
  - id: '{{discover_model_outputs.id}}'
    use: Encrypting model outputs can prevent adversaries from discovering sensitive
      information about the AI-enabled system or its operations.
  ml-lifecycle:
  - Data Preparation
  - ML Model Engineering
  - Deployment
  category:
  - Technical - Cyber
  created_date: 2023-04-12
  modified_date: 2025-12-23

- &code_signing
  id: AML.M0013
  name: Code Signing
  description: Enforce binary and application integrity with digital signature verification
    to prevent untrusted code from executing. Adversaries can embed malicious code
    in AI software or models. Enforcement of code signing can prevent the compromise
    of the AI supply chain and prevent execution of malicious code.
  object-type: mitigation
  ATT&CK-reference:
    id: M1045
    url: https://attack.mitre.org/mitigations/M1045/
  techniques:
  - id: '{{unsafe_ml_artifacts.id}}'
    use: 'Prevent execution of ML artifacts that are not properly signed.

      '
  - id: '{{supply_chain_software.id}}'
    use: 'Enforce properly signed drivers and ML software frameworks.

      '
  - id: '{{supply_chain_model.id}}'
    use: 'Enforce properly signed model files.

      '
  - id: '{{backdoor_model.id}}'
    use: Code signing provides a guarantee that the model has not been manipulated
      after signing took place.
  - id: '{{poison_model.id}}'
    use: Code signing provides a guarantee that the model has not been manipulated
      after signing took place.
  - id: '{{inject_payload.id}}'
    use: Code signing provides a guarantee that the model has not been manipulated
      after signing took place.
  - id: '{{embed_malware.id}}'
    use: Code signing provides a guarantee that the model has not been manipulated
      after signing took place.
  - id: '{{malicious_package.id}}'
    use: Code signing provides a guarantee that the software package has not been
      manipulated after signing took place.
  ml-lifecycle:
  - Deployment
  category:
  - Technical - Cyber
  created_date: 2023-04-12
  modified_date: 2025-12-23

- &verify_ml_artifacts
  id: AML.M0014
  name: Verify AI Artifacts
  description: Verify the cryptographic checksum of all AI artifacts to verify that
    the file was not modified by an attacker.
  object-type: mitigation
  techniques:
  - id: '{{publish_poisoned_data.id}}'
    use: 'Determine validity of published data in order to avoid using poisoned data
      that introduces vulnerabilities.

      '
  - id: '{{unsafe_ml_artifacts.id}}'
    use: Introduce proper checking of signatures to ensure that unsafe AI artifacts
      will not be executed in the system.
  - id: '{{supply_chain.id}}'
    use: Introduce proper checking of signatures to ensure that unsafe AI artifacts
      will not be introduced to the system.
  - id: '{{supply_chain_data.id}}'
    use: Introduce proper checking of signatures to ensure that unsafe AI data will
      not be introduced to the system.
  - id: '{{acquire_ml_artifacts_model.id}}'
    use: Introduce proper checking of signatures to ensure that unsafe AI models will
      not be introduced to the system.
  - id: '{{user_execution.id}}'
    use: Introduce proper checking of signatures to ensure that unsafe AI artifacts
      will not be executed in the system.
  ml-lifecycle:
  - Business and Data Understanding
  - Data Preparation
  - ML Model Engineering
  category:
  - Technical - Cyber
  created_date: 2023-04-12
  modified_date: 2025-12-23

- &adv_input_detection
  id: AML.M0015
  name: Adversarial Input Detection
  description: 'Detect and block adversarial inputs or atypical queries that deviate
    from known benign behavior, exhibit behavior patterns observed in previous attacks
    or that come from potentially malicious IPs.

    Incorporate adversarial detection algorithms into the AI system prior to the AI
    model.'
  object-type: mitigation
  techniques:
  - id: '{{evade_model.id}}'
    use: 'Prevent an attacker from introducing adversarial data into the system.

      '
  - id: '{{craft_adv_blackbox.id}}'
    use: 'Monitor queries and query patterns to the target model, block access if
      suspicious queries are detected.

      '
  - id: '{{ml_dos.id}}'
    use: 'Assess queries before inference call or enforce timeout policy for queries
      which consume excessive resources.

      '
  - id: '{{erode_integrity.id}}'
    use: 'Incorporate adversarial input detection into the pipeline before inputs
      reach the model.

      '
  - id: '{{craft_adv.id}}'
    use: Incorporate adversarial input detection to block malicious inputs at inference
      time.
  - id: '{{craft_adv_whitebox.id}}'
    use: Incorporate adversarial input detection to block malicious inputs at inference
      time.
  - id: '{{craft_adv_transfer.id}}'
    use: Incorporate adversarial input detection to block malicious inputs at inference
      time.
  - id: '{{craft_adv_trigger.id}}'
    use: Incorporate adversarial input detection to block malicious inputs at inference
      time.
  - id: '{{craft_adv_manual.id}}'
    use: Incorporate adversarial input detection to block malicious inputs at inference
      time.
  ml-lifecycle:
  - Data Preparation
  - ML Model Engineering
  - ML Model Evaluation
  - Deployment
  - Monitoring and Maintenance
  category:
  - Technical - ML
  created_date: 2023-04-12
  modified_date: 2025-12-23

- &vuln_scanning
  id: AML.M0016
  name: Vulnerability Scanning
  description: 'Vulnerability scanning is used to find potentially exploitable software
    vulnerabilities to remediate them.


    File formats such as pickle files that are commonly used to store AI models can
    contain exploits that allow for arbitrary code execution.

    These files should be scanned for potentially unsafe calls, which could be used
    to execute code, create new processes, or establish networking capabilities.

    Adversaries may embed malicious code in model corrupt model files, so scanners
    should be capable of working with models that cannot be fully de-serialized.

    Model artifacts, downstream products produced by models, and external software
    dependencies should be scanned for known vulnerabilities.'
  object-type: mitigation
  ATT&CK-reference:
    id: M1016
    url: https://attack.mitre.org/mitigations/M1016/
  techniques:
  - id: '{{unsafe_ml_artifacts.id}}'
    use: Vulnerability scanning can help identify malicious AI artifacts, such as
      models or data, and prevent user execution.
  - id: '{{malicious_package.id}}'
    use: Vulnerability scanning can help identify malicious packages and prevent user
      execution.
  - id: '{{user_execution.id}}'
    use: Vulnerability scanning can help identify malicious binaries and prevent user
      execution.
  ml-lifecycle:
  - ML Model Engineering
  - Data Preparation
  category:
  - Technical - Cyber
  created_date: 2023-04-12
  modified_date: 2025-12-23

- &distribution_methods
  id: AML.M0017
  name: AI Model Distribution Methods
  description: 'Deploying AI models to edge devices can increase the attack surface
    of the system.

    Consider serving models in the cloud to reduce the level of access the adversary
    has to the model.

    Also consider computing features in the cloud to prevent gray-box attacks, where
    an adversary has access to the model preprocessing methods.'
  object-type: mitigation
  techniques:
  - id: '{{full_access.id}}'
    use: 'Not distributing the model in software to edge devices, can limit an adversary''s
      ability to gain full access to the model.

      '
  - id: '{{craft_adv_whitebox.id}}'
    use: 'With full access to the model, an adversary could perform white-box attacks.

      '
  - id: '{{supply_chain_model.id}}'
    use: 'An adversary could repackage the application with a malicious version of
      the model.

      '
  - id: '{{ip_theft.id}}'
    use: Avoiding  the deployment of models to edge devices reduces an adversary's
      potential access to models or AI artifacts.
  - id: '{{ml_artifact_collection.id}}'
    use: Avoiding the deployment of models to edge devices reduces the attack surface
      and can prevent adversary artifact collection.
  - id: '{{discover_model_outputs.id}}'
    use: Avoiding the deployment of models to edge devices reduces an adversary's
      ability to collect sensitive information about the model outputs.
  ml-lifecycle:
  - Deployment
  category:
  - Policy
  created_date: 2023-04-12
  modified_date: 2025-12-23

- &user_training
  id: AML.M0018
  name: User Training
  description: 'Educate AI model developers to on AI supply chain risks and potentially
    malicious AI artifacts.

    Educate users on how to identify deepfakes and phishing attempts.'
  object-type: mitigation
  ATT&CK-reference:
    id: M1017
    url: https://attack.mitre.org/mitigations/M1017/
  techniques:
  - id: '{{user_execution.id}}'
    use: 'Training users to be able to identify attempts at manipulation will make
      them less susceptible to performing techniques that cause the execution of malicious
      code.

      '
  - id: '{{unsafe_ml_artifacts.id}}'
    use: 'Train users to identify attempts of manipulation to prevent them from running
      unsafe code which when executed could develop unsafe artifacts. These artifacts
      may have a detrimental effect on the system.

      '
  - id: '{{phishing.id}}'
    use: Train users to identify phishing attempts by an adversary to reduce the risk
      of successful spearphishing, social engineering, and other techniques that involve
      user interaction.
  - id: '{{llm_phishing.id}}'
    use: Train users to identify phishing attempts and understand that AI can be used
      to generate targeted and convincing messages.
  - id: '{{malicious_package.id}}'
    use: Train users to identify attempts of manipulation to prevent them from running
      unsafe code from external packages.
  ml-lifecycle:
  - Business and Data Understanding
  - Data Preparation
  - ML Model Engineering
  - ML Model Evaluation
  - Deployment
  - Monitoring and Maintenance
  category:
  - Policy
  created_date: 2023-04-12
  modified_date: 2025-12-23

- &control_access_prod
  id: AML.M0019
  name: Control Access to AI Models and Data in Production
  description: 'Require users to verify their identities before accessing a production
    model.

    Require authentication for API endpoints and monitor production model queries
    to ensure compliance with usage policies and to prevent model misuse.

    '
  object-type: mitigation
  techniques:
  - id: '{{inference_api.id}}'
    use: 'Adversaries can use unrestricted API access to gain information about a
      production system, stage attacks, and introduce malicious data to the system.

      '
  - id: '{{exfiltrate_via_api.id}}'
    use: 'Adversaries can use unrestricted API access to build a proxy training dataset
      and reveal private information.

      '
  - id: '{{cost_harvesting.id}}'
    use: Access controls can limit API access and prevent cost harvesting.
  - id: '{{craft_adv.id}}'
    use: Access controls on model APIs can restricts an adversary's access required
      to generate adversarial data.
  - id: '{{craft_adv_blackbox.id}}'
    use: Access controls on model APIs can deny adversaries the access required for
      black-box optimization methods.
  - id: '{{train_proxy_model.id}}'
    use: Access controls on models APIs can reduce an adversary's ability to produce
      an accurate proxy model.
  - id: '{{ml_dos.id}}'
    use: Access controls on model APIs can prevent an adversary from excessively querying
      and disabling the system.
  - id: '{{llm_prompt_injection.id}}'
    use: Use access controls in production to prevent adversaries from injecting malicious
      prompts.
  - id: '{{chaff_data.id}}'
    use: Authentication on production models can help prevent anonymous chaff data
      spam.
  - id: '{{verify_attack.id}}'
    use: Use access controls in production to prevent adversary's ability to verify
      attack efficacy.
  - id: '{{discover_model_outputs.id}}'
    use: Controlling access to the model in production can help prevent adversaries
      from inferring information from the model outputs.
  ml-lifecycle:
  - Deployment
  - Monitoring and Maintenance
  category:
  - Policy
  created_date: 2024-01-12
  modified_date: 2025-12-23

- &gen_ai_guardrails
  id: AML.M0020
  name: Generative AI Guardrails
  description: 'Guardrails are safety controls that are placed between a generative
    AI model and the output shared with the user to prevent undesired inputs and outputs.

    Guardrails can take the form of validators such as filters, rule-based logic,
    or regular expressions, as well as AI-based approaches, such as classifiers and
    utilizing LLMs, or named entity recognition (NER) to evaluate the safety of the
    prompt or response. Domain specific methods can be employed to reduce risks in
    a variety of areas such as etiquette, brand damage, jailbreaking, false information,
    code exploits, SQL injections, and data leakage.'
  object-type: mitigation
  techniques:
  - id: '{{llm_jailbreak.id}}'
    use: Guardrails can prevent harmful inputs that can lead to a jailbreak.
  - id: '{{llm_meta_prompt.id}}'
    use: Guardrails can prevent harmful inputs that can lead to meta prompt extraction.
  - id: '{{llm_plugin_compromise.id}}'
    use: Guardrails can prevent harmful inputs that can lead to plugin compromise,
      and they can detect PII in model outputs.
  - id: '{{llm_prompt_injection.id}}'
    use: Guardrails can prevent harmful inputs that can lead to prompt injection.
  - id: '{{llm_data_leakage.id}}'
    use: Guardrails can detect sensitive data and PII in model outputs.
  - id: '{{supply_chain.id}}'
    use: Guardrails can detect harmful code in model outputs.
  - id: '{{llm_prompt_self_replication.id}}'
    use: Guardrails can help prevent replication attacks in model inputs and outputs.
  - id: '{{discover_llm_hallucinations.id}}'
    use: Guardrails can help block hallucinated content that appears in model output.
  ml-lifecycle:
  - ML Model Engineering
  - ML Model Evaluation
  - Deployment
  category:
  - Technical - ML
  created_date: 2025-03-12
  modified_date: 2025-12-23

- &gen_ai_guidelines
  id: AML.M0021
  name: Generative AI Guidelines
  description: 'Guidelines are safety controls that are placed between user-provided
    input and a generative AI model to help direct the model to produce desired outputs
    and prevent undesired outputs.


    Guidelines can be implemented as instructions appended to all user prompts or
    as part of the instructions in the system prompt. They can define the goal(s),
    role, and voice of the system, as well as outline safety and security parameters.'
  object-type: mitigation
  techniques:
  - id: '{{llm_jailbreak.id}}'
    use: Model guidelines can instruct the model to refuse a response to unsafe inputs.
  - id: '{{llm_meta_prompt.id}}'
    use: Model guidelines can instruct the model to refuse a response to unsafe inputs.
  - id: '{{llm_plugin_compromise.id}}'
    use: Model guidelines can instruct the model to refuse a response to unsafe inputs.
  - id: '{{llm_prompt_injection.id}}'
    use: Model guidelines can instruct the model to refuse a response to unsafe inputs.
  - id: '{{llm_data_leakage.id}}'
    use: Model guidelines can instruct the model to refuse a response to unsafe inputs.
  - id: '{{llm_prompt_self_replication.id}}'
    use: Guidelines can help instruct the model to produce more secure output, preventing
      the model from generating self-replicating outputs.
  - id: '{{discover_llm_hallucinations.id}}'
    use: Guidelines can instruct the model to avoid producing hallucinated content.
  ml-lifecycle:
  - ML Model Engineering
  - ML Model Evaluation
  - Deployment
  category:
  - Technical - ML
  created_date: 2025-03-12
  modified_date: 2025-12-23

- &gen_ai_alignment
  id: AML.M0022
  name: Generative AI Model Alignment
  description: 'When training or fine-tuning a generative AI model it is important
    to utilize techniques that improve model alignment with safety, security, and
    content policies.


    The fine-tuning process can potentially remove built-in safety mechanisms in a
    generative AI model, but utilizing techniques such as Supervised Fine-Tuning,
    Reinforcement Learning from Human Feedback or AI Feedback, and Targeted Safety
    Context Distillation can improve the safety and alignment of the model.'
  object-type: mitigation
  techniques:
  - id: '{{llm_jailbreak.id}}'
    use: Model alignment can improve the parametric safety of a model by guiding it
      away from unsafe prompts and responses.
  - id: '{{llm_meta_prompt.id}}'
    use: Model alignment can improve the parametric safety of a model by guiding it
      away from unsafe prompts and responses.
  - id: '{{llm_plugin_compromise.id}}'
    use: Model alignment can improve the parametric safety of a model by guiding it
      away from unsafe prompts and responses.
  - id: '{{llm_prompt_injection.id}}'
    use: Model alignment can improve the parametric safety of a model by guiding it
      away from unsafe prompts and responses.
  - id: '{{llm_data_leakage.id}}'
    use: Model alignment can improve the parametric safety of a model by guiding it
      away from unsafe prompts and responses.
  - id: '{{llm_prompt_self_replication.id}}'
    use: Model alignment can increase the security of models to self replicating prompt
      attacks.
  - id: '{{discover_llm_hallucinations.id}}'
    use: Model alignment can help steer the model away from hallucinated content.
  ml-lifecycle:
  - ML Model Engineering
  - ML Model Evaluation
  - Deployment
  category:
  - Technical - ML
  created_date: 2025-03-12
  modified_date: 2025-12-23

- &ai_bom
  id: AML.M0023
  name: AI Bill of Materials
  description: 'An AI Bill of Materials (AI BOM) contains a full listing of artifacts
    and resources that were used in building the AI. The AI BOM can help mitigate
    supply chain risks and enable rapid response to reported vulnerabilities.


    This can include maintaining dataset provenance, i.e. a detailed history of datasets
    used for AI applications. The history can include information about the dataset
    source as well as well as a complete record of any modifications.'
  object-type: mitigation
  techniques:
  - id: '{{unsafe_ml_artifacts.id}}'
    use: An AI BOM can help users identify untrustworthy model artifacts.
  - id: '{{publish_poisoned_data.id}}'
    use: An AI BOM can help users identify untrustworthy model artifacts.
  - id: '{{poison_data.id}}'
    use: An AI BOM can help users identify untrustworthy model artifacts.
  - id: '{{publish_poisoned_model.id}}'
    use: An AI BOM can help users identify untrustworthy model artifacts.
  - id: '{{malicious_package.id}}'
    use: An AI BOM can help users identify untrustworthy software dependencies.
  - id: '{{user_execution.id}}'
    use: An AI BOM can help users identify untrustworthy binaries.
  - id: '{{supply_chain.id}}'
    use: An AI BOM can help users identify untrustworthy components of their AI supply
      chain.
  ml-lifecycle:
  - Business and Data Understanding
  - Data Preparation
  - ML Model Engineering
  category:
  - Policy
  created_date: 2025-03-12
  modified_date: 2025-12-23

- &ai_telemetry_logging
  id: AML.M0024
  name: AI Telemetry Logging
  description: 'Implement logging of inputs and outputs of deployed AI models. When
    deploying AI agents, implement logging of the intermediate steps of agentic actions
    and decisions, data access and tool use, and identity of the agent. Monitoring
    logs can help to detect security threats and mitigate impacts.


    Additionally, having logging enabled can discourage adversaries who want to remain
    undetected from utilizing AI resources.'
  object-type: mitigation
  techniques:
  - id: '{{exfiltrate_via_api.id}}'
    use: Telemetry logging can help identify if sensitive data has been exfiltrated.
  - id: '{{membership_inference.id}}'
    use: Telemetry logging can help identify if sensitive data has been exfiltrated.
  - id: '{{model_inversion.id}}'
    use: Telemetry logging can help identify if sensitive data has been exfiltrated.
  - id: '{{extract_model.id}}'
    use: Telemetry logging can help identify if sensitive data has been exfiltrated.
  - id: '{{replicate_model.id}}'
    use: Telemetry logging can help identify if a proxy training dataset has been
      exfiltrated.
  - id: '{{inference_api.id}}'
    use: Telemetry logging can help audit API usage of the model.
  - id: '{{ml_service.id}}'
    use: Telemetry logging can help identify if sensitive model information has been
      sent to an attacker.
  - id: '{{llm_prompt_injection.id}}'
    use: Telemetry logging can help identify if unsafe prompts have been submitted
      to the LLM.
  - id: '{{pi_direct.id}}'
    use: Telemetry logging can help identify if unsafe prompts have been submitted
      to the LLM.
  - id: '{{pi_indirect.id}}'
    use: Telemetry logging can help identify if unsafe prompts have been submitted
      to the LLM.
  - id: '{{pi_triggered.id}}'
    use: Telemetry logging can help identify if unsafe prompts have been submitted
      to the LLM.
  - id: '{{llm_plugin_compromise.id}}'
    use: Log AI agent tool invocations to detect malicious calls.
  - id: '{{exfil_agent_tool.id}}'
    use: Log AI agent tool invocations to detect malicious calls.
  - id: '{{data_destruction_via_tool.id}}'
    use: Log AI agent tool invocations to detect malicious calls.
  - id: '{{data_from_ai.id}}'
    use: Log requests to AI services to detect malicious queries for data.
  - id: '{{rag_data_harvest.id}}'
    use: Log requests to AI services to detect malicious queries for data.
  - id: '{{agent_tool_harvest.id}}'
    use: Log requests to AI services to detect malicious queries for data.
  ml-lifecycle:
  - Deployment
  - Monitoring and Maintenance
  category:
  - Technical - Cyber
  created_date: 2025-03-12
  modified_date: 2025-12-23

- &maintain_dataset_provenance
  id: AML.M0025
  name: Maintain AI Dataset Provenance
  description: Maintain a detailed history of datasets used for AI applications. The
    history should include information about the dataset's source as well as a complete
    record of any modifications.
  object-type: mitigation
  techniques:
  - id: '{{supply_chain_data.id}}'
    use: Dataset provenance can protect against supply chain compromise of data.
  - id: '{{poison_data.id}}'
    use: Dataset provenance can protect against poisoning of training data
  - id: '{{poison_model.id}}'
    use: Dataset provenance can protect against poisoning of models.
  - id: '{{publish_poisoned_data.id}}'
    use: Maintaining a detailed history of datasets can help identify use of poisoned
      datasets from public sources.
  - id: '{{erode_integrity_dataset.id}}'
    use: Maintaining dataset provenance can help identify adverse changes to the data.
  ml-lifecycle:
  - Data Preparation
  - Business and Data Understanding
  category:
  - Technical - ML
  created_date: 2025-03-12
  modified_date: 2025-12-23

- &agent_config_priv
  id: AML.M0026
  name: Privileged AI Agent Permissions Configuration
  description: AI agents may be granted elevated privileges above that of a normal
    user to enable desired workflows. When deploying a privileged AI agent, or an
    agent that interacts with multiple users, it is important to implement robust
    policies and controls on permissions of the privileged agent. These controls include
    Role-Based Access Controls (RBAC), Attribute-Based Access Controls (ABAC), and
    the principle of least privilege so that the agent is only granted the necessary
    permissions to access tools and resources required to accomplish its designated
    task(s).
  object-type: mitigation
  techniques:
  - id: '{{exfil_agent_tool.id}}'
    use: Configuring privileged AI agents with proper access controls for tool use
      can limit an adversary's ability to abuse tool invocations if the agent is compromised.
  - id: '{{llm_plugin_compromise.id}}'
    use: Configuring privileged AI agents with proper access controls for tool use
      can limit an adversary's ability to abuse tool invocations if the agent is compromised.
  - id: '{{data_from_ai.id}}'
    use: Configuring privileged AI agents with proper access controls can limit an
      adversary's ability to collect data from AI services if the agent is compromised.
  - id: '{{rag_data_harvest.id}}'
    use: Configuring privileged AI agents with proper access controls can limit an
      adversary's ability to collect data from RAG Databases if the agent is compromised.
  - id: '{{agent_tool_harvest.id}}'
    use: Configuring privileged AI agents with proper access controls can limit an
      adversary's ability to collect data from agent tool invocation if the agent
      is compromised.
  - id: '{{rag_credentials.id}}'
    use: Configuring privileged AI agents with proper access controls can limit an
      adversary's ability to harvest credentials from RAG Databases if the agent is
      compromised.
  - id: '{{data_destruction_via_tool.id}}'
    use: Configuring privileged AI agents with proper access controls for tool use
      can limit an adversary's ability to abuse tool invocations if the agent is compromised.
  ml-lifecycle:
  - Deployment
  category:
  - Technical - Cyber
  created_date: 2025-10-29
  modified_date: 2025-12-23

- &agent_config_user
  id: AML.M0027
  name: Single-User AI Agent Permissions Configuration
  description: When deploying an AI agent that acts as a representative of a user
    and performs actions on their behalf, it is important to implement robust policies
    and controls on permissions and lifecycle management of the agent. Lifecycle management
    involves establishing identity, protocols for access management, and decommissioning
    of the agent when its role is no longer needed. Controls should also include the
    principle of least privilege and delegated access from the user account. When
    acting as a representative of a user, the AI agent should not be granted permissions
    that the user would not be granted within the system or organization.
  object-type: mitigation
  techniques:
  - id: '{{exfil_agent_tool.id}}'
    use: Configuring AI agents with permissions that are inherited from the user for
      tool use can limit an adversary's ability to abuse tool invocations if the agent
      is compromised.
  - id: '{{llm_plugin_compromise.id}}'
    use: Configuring AI agents with permissions that are inherited from the user for
      tool use can limit an adversary's ability to abuse tool invocations if the agent
      is compromised.
  - id: '{{data_from_ai.id}}'
    use: Configuring AI agents with permissions that are inherited from the user can
      limit an adversary's ability to collect data from AI services if the agent is
      compromised.
  - id: '{{rag_data_harvest.id}}'
    use: Configuring AI agents with permissions that are inherited from the user can
      limit an adversary's ability to collect data from RAG Databases if the agent
      is compromised.
  - id: '{{agent_tool_harvest.id}}'
    use: Configuring AI agents with permissions that are inherited from the user can
      limit an adversary's ability to collect data from agent tool invocation if the
      agent is compromised.
  - id: '{{rag_credentials.id}}'
    use: Configuring AI agents with permissions that are inherited from the user can
      limit an adversary's ability to harvest credentials from RAG Databases if the
      agent is compromised.
  - id: '{{data_destruction_via_tool.id}}'
    use: Configuring AI agents with permissions that are inherited from the user for
      tool use can limit an adversary's ability to abuse tool invocations if the agent
      is compromised.
  ml-lifecycle:
  - Deployment
  category:
  - Technical - Cyber
  created_date: 2025-10-29
  modified_date: 2025-12-23

- &agent_config_tools
  id: AML.M0028
  name: AI Agent Tools Permissions Configuration
  description: When deploying tools that will be shared across multiple AI agents,
    it is important to implement robust policies and controls on permissions for the
    tools. These controls include applying the principle of least privilege along
    with delegated access, where the tools receive the permissions, identities, and
    restrictions of the AI agent calling them. These configurations may be implemented
    either in MCP servers which connect the agents to the tools calling them or, in
    more complex cases, directly in the configuration files of the tool.
  object-type: mitigation
  techniques:
  - id: '{{llm_plugin_compromise.id}}'
    use: Configuring AI Agent tools with access controls inherited from the user or
      the AI Agent invoking the tool can limit an adversary's capabilities within
      a system, including their ability to abuse tool invocations and access sensitive
      data.
  - id: '{{data_from_ai.id}}'
    use: Configuring AI Agent tools with access controls inherited from the user or
      the AI Agent invoking the tool can limit adversary's access to sensitive data.
  - id: '{{agent_tool_harvest.id}}'
    use: Configuring AI Agent tools with access controls that are inherited from the
      user or the AI Agent invoking the tool can limit adversary's access to sensitive
      data.
  - id: '{{data_destruction_via_tool.id}}'
    use: Configuring AI Agent tools with access controls inherited from the user or
      the AI Agent invoking the tool can limit an adversary's capabilities within
      a system, including their ability to abuse tool invocations to destroy data.
  - id: '{{exfil_agent_tool.id}}'
    use: Configuring AI Agent tools with access controls inherited from the user or
      the AI Agent invoking the tool can limit an adversary's capabilities within
      a system, including their ability to abuse tool invocations and exfiltrate sensitive
      data.
  ml-lifecycle:
  - Deployment
  category:
  - Technical - Cyber
  created_date: 2025-10-29
  modified_date: 2025-12-23

- &hitl_agent_actions
  id: AML.M0029
  name: Human In-the-Loop for AI Agent Actions
  description: "Systems should require the user or another human stakeholder to approve\
    \ AI agent actions before the agent takes them. The human approver may be technical\
    \ staff or business unit SMEs depending on the use case. Separate tools, such\
    \ as dedicated audit agents, may assist human approval, but final adjudication\
    \ should be conducted by a human decision-maker. \n\nThe security benefits from\
    \ Human In-the-Loop policies may be at odds with operational overhead costs of\
    \ additional approvals. To ease this, Human In-the-Loop policies should follow\
    \ the degree of consequence of the task at hand. Minor, repetitive tasks performed\
    \ by agents accessing basic tools may only require minimal human oversight, while\
    \ agents employed in systems with significant consequences may necessitate approval\
    \ from multiple stakeholders diversified across multiple organizations."
  object-type: mitigation
  techniques:
  - id: '{{exfil_agent_tool.id}}'
    use: Requiring user confirmation of AI agent tool invocations can prevent the
      automatic execution of tools by an adversary.
  - id: '{{llm_plugin_compromise.id}}'
    use: Requiring user confirmation of AI agent tool invocations can prevent the
      automatic execution of tools by an adversary.
  - id: '{{data_destruction_via_tool.id}}'
    use: Requiring user confirmation of AI agent tool invocations can prevent the
      automatic execution of tools by an adversary.
  ml-lifecycle:
  - Deployment
  category:
  - Technical - ML
  created_date: 2025-10-29
  modified_date: 2025-12-23

- &restrict_agent_tool_untrusted
  id: AML.M0030
  name: Restrict AI Agent Tool Invocation on Untrusted Data
  description: 'Untrusted data can contain prompt injections that invoke an AI agent''s
    tools, potentially causing confidentiality, integrity or availability violations.
    It is recommended that tool invocation be restricted or limited when untrusted
    data enters the LLM''s context.


    The degree to which tool invocation is restricted may depend on the potential
    consequences of the action. Consider blocking the automatic invocation of tools
    or requiring user confirmation once untrusted data enters the LLM''s context.
    For high consequence actions, consider always requiring user confirmation.'
  object-type: mitigation
  techniques:
  - id: '{{llm_plugin_compromise.id}}'
    use: Restricting the automatic tool use when untrusted data is present can prevent
      adversaries from invoking tools via prompt injections.
  - id: '{{exfil_agent_tool.id}}'
    use: Restricting the automatic tool use when untrusted data is present can prevent
      adversaries from invoking tools via prompt injections.
  - id: '{{data_destruction_via_tool.id}}'
    use: Restricting the automatic tool use when untrusted data is present can prevent
      adversaries from invoking tools via prompt injections.
  ml-lifecycle:
  - Deployment
  category:
  - Technical - ML
  created_date: 2025-10-29
  modified_date: 2025-12-23

- &mem_harden
  id: AML.M0031
  name: Memory Hardening
  description: Memory Hardening involves developing trust boundaries and secure processes
    for how an AI agent stores and accesses memory and context. This may be implemented
    using a combination of strategies including restricting an agent's ability to
    store memories by requiring external authentication and validation for memory
    updates, performing semantic integrity checks on retrieved memories before agents
    execute actions, and implementing controls for monitoring of memory and remediation
    processes for poisoned memory.
  object-type: mitigation
  techniques:
  - id: '{{llm_context.id}}'
    use: Memory hardening can help protect LLM memory from manipulation and prevent
      poisoned memories from executing.
  - id: '{{llm_memory_poisoning.id}}'
    use: Memory hardening can help protect LLM memory from manipulation and prevent
      poisoned memories from executing.
  ml-lifecycle:
  - ML Model Engineering
  - Deployment
  - Monitoring and Maintenance
  category:
  - Technical - ML
  created_date: 2025-10-29
  modified_date: 2025-12-20

- &seg_agent_components
  id: AML.M0032
  name: Segmentation of AI Agent Components
  description: Define security boundaries around agentic tools and data sources with
    methods such as API access, container isolation, code execution sandboxing, and
    rate limiting of tool invocation. This restricts untrusted processes or potential
    compromises from spreading throughout the system.
  object-type: mitigation
  techniques:
  - id: '{{llm_plugin_compromise.id}}'
    use: Segmentation can prevent adversaries from utilizing tools in an agentic workflow
      to perform unsafe actions that affect other components.
  - id: '{{exfil_agent_tool.id}}'
    use: Segmentation can prevent adversaries from utilizing tools in an agentic workflow
      to compromise sensitive data sources.
  - id: '{{agent_tool_credentials.id}}'
    use: Segmentation can prevent adversaries from utilizing tools in an agentic workflow
      to harvest credentials.
  - id: '{{data_from_ai.id}}'
    use: Segmentation can prevent adversaries from utilizing tools in an agentic workflow
      to collect sensitive data from AI services.
  - id: '{{rag_data_harvest.id}}'
    use: Segmentation can prevent adversaries from utilizing tools in an agentic workflow
      to collect sensitive data from RAG databases.
  - id: '{{agent_tool_harvest.id}}'
    use: Segmentation can prevent adversaries from utilizing tools in an agentic workflow
      to collect sensitive data.
  ml-lifecycle:
  - Deployment
  - Business and Data Understanding
  category:
  - Technical - Cyber
  created_date: 2025-11-25
  modified_date: 2025-12-18

- &io_val_agent_components
  id: AML.M0033
  name: Input and Output Validation for AI Agent Components
  description: Implement validation on inputs and outputs for the tools and data sources
    used by AI agents. Validation includes enforcing a common data format, schema
    validation, checks for sensitive or prohibited information leakage, and data sanitization
    to remove potential injections or unsafe code. Input and output validation can
    help prevent compromises from spreading in AI-enabled systems and can help secure
    the workflow when multiple components are chained together. Validation should
    be performed external to the AI agent.
  object-type: mitigation
  techniques:
  - id: '{{llm_plugin_compromise.id}}'
    use: Validation can prevent adversaries from utilizing tools in an agentic workflow
      to generate unsafe output.
  - id: '{{exfil_agent_tool.id}}'
    use: Validation can prevent adversaries from utilizing tools in an agentic workflow
      to compromise sensitive data sources.
  - id: '{{llm_prompt_injection.id}}'
    use: Validation can prevent adversaries from executing prompt injections that
      could affect agentic workflows.
  - id: '{{pi_direct.id}}'
    use: Validation can prevent adversaries from executing prompt injections that
      could affect agentic workflows.
  - id: '{{pi_indirect.id}}'
    use: Validation can prevent adversaries from executing prompt injections that
      could affect agentic workflows.
  - id: '{{pi_triggered.id}}'
    use: Validation can prevent adversaries from executing prompt injections that
      could affect agentic workflows.
  ml-lifecycle:
  - Business and Data Understanding
  - Data Preparation
  - Deployment
  category:
  - Technical - ML
  created_date: 2025-11-25
  modified_date: 2025-12-18

- &deepfake_detection
  id: AML.M0034
  name: Deepfake Detection
  description: "Apply deepfake detection algorithms against any untrusted or user-provided\
    \ data, especially in impactful applications such as biometric verification, to\
    \ block generated content.\n\nDetectors may use a combination of approaches, including:\n\
    -\tAI models trained to differentiate between real and deepfake content.\n-\t\
    Identifying common inconsistencies in deepfake content, such as unnatural facial\
    \ movements, audio mismatches, or pixel-level artifacts.\n-\tBiometrics analysis,\
    \ such blinking, eye movements, and microexpressions."
  object-type: mitigation
  techniques:
  - id: '{{gen_deepfake.id}}'
    use: Deepfake detection can be used to identify and block generated content.
  - id: '{{phishing.id}}'
    use: Deepfake detection can be used to identify and block phishing attempts that
      use generated content.
  - id: '{{llm_phishing.id}}'
    use: Deepfake detection can be used to identify and block phishing attempts that
      use generated content.
  - id: '{{evade_model.id}}'
    use: Deepfake detection can be used to identify and block generated content.
  ml-lifecycle:
  - Deployment
  - Monitoring and Maintenance
  - ML Model Evaluation
  - ML Model Engineering
  category:
  - Technical - ML
  created_date: 2025-11-25
  modified_date: 2025-11-25
