---

- id: AML.CS0000
  name: Evasion of Deep Learning detector for malware C&C traffic
  object-type: case-study
  summary: |
    Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
    Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
    Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
  incident-date: 2020-01-01
  procedure:
    - tactic: *id_reconnaissance
      technique: "{{victim_research_preprint.id}}"
      description: |
        We identified a machine learning based approach to malicious URL detection as a representative approach and potential target from the paper "URLNet: Learning a URL representation with deep learning for malicious URL detection" [1], which was found on arXiv (a pre-print repository).
    - tactic: *id_resource_development
      technique: "{{acquire_ml_artifacts_data.id}}"
      description: |
        We acquired a similar dataset to the target production model.
    - tactic: *id_resource_development
      technique: "{{train_proxy_model.id}}"
      description: |
        We built a model that was trained on a similar dataset as the production model.
        We trained the model on ~ 33 million benign and ~ 27 million malicious HTTP packet headers.
        Evaluation showed a true positive rate of ~ 99% and false positive rate of ~0.01%, on average.
        Testing the model with a HTTP packet header from known malware command and control traffic samples was detected as malicious with high confidence (> 99%).
    - tactic: "{{ml_attack_staging.id}}"
      technique: "{{craft_adv_manual.id}}"
      description: |
        We crafted evasion samples by removing fields from packet header which are typically not used for C&C communication (e.g. cache-control, connection, etc.)
    - tactic: "{{ml_attack_staging.id}}"
      technique: "{{verify_attack.id}}"
      description: |
        We queried the model with our adversarial examples and adjusted them until the model was evaded.
    - tactic: *id_impact
      technique: "{{evade_model.id}}"
      description: |
        With the crafted samples we performed online evasion of the ML-based spyware detection model.
        The crafted packets were identified as benign with >80% confidence.
        This evaluation demonstrates that adversaries are able to bypass advanced ML detection techniques, by crafting samples that are misclassified by an ML model.
  reported-by:
    - Palo Alto Networks (Network Security AI Research Team)
  references:
    - 'Le, Hung, et al. "URLNet: Learning a URL representation with deep learning for malicious URL detection." arXiv preprint arXiv:1802.03162 (2018).'

- id: AML.CS0001
  name: Botnet Domain Generation Algorithm (DGA) Detection Evasion
  object-type: case-study
  summary: |
    The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
    It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
    The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
  incident-date: 2020-01-01
  procedure:
    - tactic: *id_reconnaissance
      technique: "{{victim_research.id}}"
      description: |
        DGA detection is a widely used technique to detect botnets in academia and industry.
        The searched for research papers related to DGA detection.
    - tactic: *id_resource_development
      technique: "{{acquire_ml_artifacts.id}}"
      description: |
        The researchers acquired a publicly available CNN-based DGA detection model [1] and tested against a well-known DGA generated domain name data sets, which includes ~50 million domain names from 64 botnet DGA families.
        The CNN-based DGA detection model shows more than 70% detection accuracy on 16 (~25%) botnet DGA families.
    - tactic: *id_resource_development
      technique: "{{develop_advml.id}}"
      description: |
        The researchers developed a generic mutation technique that requires a minimal number of iterations.
    - tactic: "{{ml_attack_staging.id}}"
      technique: "{{craft_adv_blackbox.id}}"
      description: |
        The researchers used the mutation technique to generate evasive domain names.
    - tactic: "{{ml_attack_staging.id}}"
      technique: "{{verify_attack.id}}"
      description: |
        Experiment results show that, after only one string is inserted once to the DGA generated domain names, the detection rate of all 16 botnet DGA families can drop to less than 25% detection accuracy.
    - tactic: *id_defense_evasion
      technique: "{{evade_model.id}}"
      description: |
        The DGA generated domain names mutated with this technique successfully evade the target DGA Detection model, allowing an adversary to continue communication with their [Command and Control]() servers.
  reported-by:
    - Palo Alto Networks (Network Security AI Research Team)
  references:
    - '[1] Yu, Bin, Jie Pan, Jiaming Hu, Anderson Nascimento, and Martine De Cock. "Character level based detection of DGA domain names." In 2018 International Joint Conference on Neural Networks (IJCNN), pp. 1-8. IEEE, 2018. Source code is available from Github: https://github.com/matthoffman/degas'

- id: AML.CS0002
  name: VirusTotal Poisoning
  object-type: case-study
  summary: |
    An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
    In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
    Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
    Interestingly enough, the compile time was the same for all the samples.
    After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
    The variants wouldn't always be executable but still classified as the same ransomware family.
  incident-date: 2020-01-01
  procedure:
    - tactic: *id_resource_development
      technique: "{{obtain_advml.id}}"
      description: |
        The actor obtained [metame](https://github.com/a0rtega/metame), a simple metamorphic code engine for arbitrary executables.
    - tactic: "{{ml_attack_staging.id}}"
      technique: "{{craft_adv.id}}"
      description: |
        The actor used a malware sample from a prevalent ransomware family as a start to create 'mutant' variants.
    - tactic: *id_initial_access
      technique: "{{supply_chain_data.id}}"
      description: |
        The actor uploaded "mutant" samples to the platform.
    - tactic: *id_persistence
      technique: "{{poison_data.id}}"
      description: |
        Several vendors started to classify the files as the ransomware family even though most of them won't run.
        The "mutant" samples poisoned the dataset the ML model(s) use to identify and classify this ransomware family.
  reported-by:
    - Christiaan Beek (@ChristiaanBeek) - McAfee Advanced Threat Research

- id: AML.CS0003
  name: Bypassing Cylance's AI Malware Detection
  object-type: case-study
  summary: |
    Researchers at Skylight were able to create a universal bypass string that
    when appended to a malicious file evades detection by Cylance's AI Malware detector.
  incident-date: 2019-09-07
  procedure:
    - tactic: *id_reconnaissance
      technique: T1594  # Search Victim-Owned Websites
      description: |
        The researchers read publicly available information about Cylance's AI Malware detector.
    - tactic: "{{ml_model_access.id}}"
      technique: "{{ml_service.id}}"
      description: |
        The researchers used Cylance's AI Malware detector and enabled verbose logging to understand the inner workings of the ML model, particularly around reputation scoring.
    - tactic: *id_resource_development
      technique: "{{develop_advml.id}}"
      description: |
        The researchers used the reputation scoring information to reverse engineer which attributes provided what level of positive or negative reputation.
        Along the way, they discovered a secondary model which was an override for the first model.
        Positive assessments from the second model overrode the decision of the core ML model.
    - tactic: "{{ml_attack_staging.id}}"
      technique: "{{craft_adv_manual.id}}"
      description: |
        Using this knowledge, the researchers fused attributes of known good files with malware to manually create adversarial malware.
    - tactic: *id_impact
      technique: "{{evade_model.id}}"
      description: |
        Due to the secondary model overriding the primary, the researchers were effectively able to bypass the ML model.
  reported-by:
    - Research and work by Adi Ashkenazy, Shahar Zini, and SkyLight Cyber team.Notified to us by Ken Luu (@devianz_)
  references:
    - https://skylightcyber.com/2019/07/18/cylance-i-kill-you/

- id: AML.CS0004
  name: Camera Hijack Attack on Facial Recognition System
  object-type: case-study
  summary: |
    This type of attack can break through the traditional live detection model
    and cause the misuse of face recognition.
  incident-date: 2020-01-01
  procedure:
    - tactic: *id_resource_development
      technique: T1583  # Acquire Infrastructure
      description: |
        The attackers bought customized low-end mobile phones.
    - tactic: *id_resource_development
      technique: T1588.002  # Obtain Capabilities: Tool
      description: |
        The attackers obtained customized android ROMs and a virtual camera application.
    - tactic: *id_resource_development
      technique: "{{obtain_advml.id}}"
      description: |
        The attackers obtained software that turns static photos into videos, adding realistic effects such as blinking eyes.
    - tactic: *id_collection
      technique: T1213  #  Data from Information Repositories
      description: |
        The attackers collected user identity information and face photos.
    - tactic: *id_resource_development
      technique: T1585  # Establish Accounts
      description: |
        The attackers registered accounts with the victims' identity information.
    - tactic: "{{ml_model_access.id}}"
      technique: "{{ml_service.id}}"
      description: |
        The attackers used the virtual camera app to present the generated video to the ML-based facial recongition product used for user verification.
    - tactic: *id_impact
      techniquue: "{{evade_model.id}}"
      description: |
        The attackers successfully evaded the face recognition system and impersonated the victim.
  reported-by:
    - Henry Xuef, Ant Group AISEC Team
  references:

- id: AML.CS0005
  name: Attack on Machine Translation Service - Google Translate, Bing Translator, and Systran Translate
  object-type: case-study
  summary: |
    Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
    A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
    Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
    These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
  incident-date: 2020-04-30
  procedure:
    - tactic: *id_reconnaissance
      technique: "{{victim_research.id}}"
      description: |
        The researchers used published research papers to identify the datasets and model architectures used by the target translaation services.
    - tactic: *id_resource_development
      technique: "{{acquire_ml_artifacts_data.id}}"
      description: |
        The researchers gathered similar datasets that the target translation services used.
    - tactic: *id_resource_development
      technique: "{{acquire_ml_artifacts_model.id}}"
      description: |
        The researchers gathered similar model architectures that the target translation services used.
    - tactic: "{{ml_model_access.id}}"
      technique: "{{inference_api.id}}"
      description: |
        They abuse a public facing application to query the model and produce machine translated sentence pairs as training data.
    - tactic: "{{ml_attack_staging.id}}"
      technique: "{{replicate_model.id}}"
      description: |
        Using these translated sentence pairs, the researchers trained a model that replicates the behavior of the target model.
    - tactic: *id_impact
      techniuqe: "{{ip_theft.id}}"
      description: |
        By replicating the model with high fidelity, the researchers demonstrated that an adversary could steal a model and violate the victim's intellectual property rights.
    - tactic: "{{ml_attack_staging.id}}"
      technique: "{{craft_adv_transfer.id}}"
      description: |
        The replicated models were used to generate adversarial examples that successfully transferred to the black-box translation services.
    - tactic: *id_impact
      technique: "{{evade_model}}"
      description: |
        The adversarial examples were used to evade the machine translation services.
  reported-by:
    - Work by Eric Wallace, Mitchell Stern, Dawn Song and reported by Kenny Song (@helloksong)
  references:
    - https://arxiv.org/abs/2004.15015
    - https://www.ericswallace.com/imitation

- id: AML.CS0006
  name: ClearviewAI Misconfiguration
  object-type: case-study
  summary: |
    Clearview AI's source code repository, though password protected, was misconfigured to allow an arbitrary user to register an account.
    This allowed an external researcher to gain access to a private code repository that contained Clearview AI production credentials, keys to cloud storage buckets containing 70K video samples, and copies of its applications and Slack tokens.
    With access to training data, a bad-actor has the ability to cause an arbitrary misclassification in the deployed model.
    These kinds of attacks illustrate that any attempt to secure ML system should be on top of "traditional" good cybersecurity hygiene such as locking down the system with least privileges, multi-factor authentication and monitoring and auditing.
  incident-date: 2020-04-16
  procedure:
    - tactic: *id_initial_access
      technique: T1078  # Valid Accounts
      description: |
        In this scenario, a security researcher gained initial access to via a valid account that was created through a misconfiguration.
  reported-by:
    - Mossab Hussein (@mossab_hussein)
  references:
    - https://techcrunch.com/2020/04/16/clearview-source-code-lapse/amp/
    - https://gizmodo.com/we-found-clearview-ais-shady-face-recognition-app-1841961772

- id: AML.CS0007
  name: GPT-2 Model Replication
  object-type: case-study
  summary: |
    OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model. Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
  incident-date: 2019-08-22
  procedure:
    - tactic: *id_reconnaissance
      technique: "{{victim_research.id}}"
      description: |
        Using the public documentation about GPT-2, ML researchers gathered information about the dataset, model architecture, and training hyper-parameters.
    - tactic: *id_resource_development
      technique: "{{acquire_ml_artifacts_model.id}}"
      description: |
        The researchers obtained a reference implementation of a similar publicly available model called Grover.
    - tactic: *id_resource_development
      technique: "{{acquire_ml_artifacts_data.id}}"
      description: |
        The researchers were able to manually recreate the dataset used in the original GPT-2 paper using the gathered documentation.
    - tactic: *id_resource_development
      technique: "{{acquire_workspaces.id}}"
      description: |
        The researchers were able to use TensorFlow Research Cloud via their academic credentials.
    - tactic: "{{ml_attack_staging.id}}"
      technique: "{{train_proxy_model.id}}"
      description: |
        The researchers modified Grover's objective function to reflect GPT-2's objective function and then trained on the dataset they curated.
        They used Grover's initial hyperparameters for training.
        This resulted in their replicated model.
  reported-by:
    - Vanya Cohen (@VanyaCohen)
    - Aaron Gokaslan (@SkyLi0n)
    - Ellie Pavlick
    - Stefanie Tellex
  references:
    - https://www.wired.com/story/dangerous-ai-open-source/
    - https://blog.usejournal.com/opengpt-2-we-replicated-gpt-2-because-you-can-too-45e34e6d36dc

- id: AML.CS0008
  name: ProofPoint Evasion
  object-type: case-study
  summary: |
    CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
  incident-date: 2019-09-09
  procedure:
    - tactic: *id_resource_development
      technique: "{{acquire_ml_artifacts.id}}"
      description: |
        The researchers first gathered the scores from the Proofpoint's ML system used in email headers by sending a large number of emails through the system and scraping the model scores exposed in the logs.
    - tactic: *id_resource_development
      technique: "{{acquire_ml_artifacts_data.id}}"
      description: |
        The researchers converted the collected scores into a dataset.
    - tactic: "{{ml_attack_staging.id}}"
      technique: "{{train_proxy_model.id}}"
      description: |
        Using these scores, the researchers replicated the ML mode by building a "shadow" aka copy-cat ML model.
    - tactic: "{{ml_attack_staging.id}}"
      technique: "{{craft_adv_whitebox.id}}"
      description: |
        Next, the ML researchers algorithmically found samples that this "offline" copy cat model.
    - tactic: "{{ml_attack_staging.id}}"
      technique: "{{craft_adv_transfer.id}}"
      description: |
        Finally, these insights from the offline model allowed the researchers to create malicious emails that received preferable scores from the real ProofPoint email protection system, hence bypassing it.
  reported-by:
    - Will Pearce (@moo_hax)
    - Nick Landers (@monoxgas)
  references:
    - https://nvd.nist.gov/vuln/detail/CVE-2019-20634
    - https://github.com/moohax/Talks/blob/master/slides/DerbyCon19.pdf
    - https://github.com/moohax/Proof-Pudding

- id: AML.CS0009
  name: Tay Poisoning
  object-type: case-study
  summary: |
    Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes. Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.
  incident-date: 2016-03-23
  procedure:
    - tactic: "{{ml_model_access.id}}"
      technique: "{{inference_api.id}}"
      description: |
        Adversaries were able to interact with Tay via a few different publicly available methods.
    - tactic: *id_initial_access
      technique: "{{supply_chain_data.id}}"
      description: |
        Tay bot used the interactions with its twitter users as training data to improve its conversations.
        Adversaries were able to coordinate with the intent of defacing Tay bot by exploiting this feedback loop.
    - tactic: *id_persistence
      technique: "{{poison_data.id}}"
      description: |
        By repeatedly interacting with Tay using racist and offensive language, they were able to bias Tay's dataset towards that language as well.
    - tactic: *id_impact
      technique: "{{erode_integrity.id}}"
      description: |
        As a result of this coordinated attack, Tay's conversation algorithms began to learn to generate reprehensible material.
        This quickly lead to its decommissioning.
  reported-by:
    - Microsoft
  references:
    - https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/
    - https://spectrum.ieee.org/tech-talk/artificial-intelligence/machine-learning/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation

- id: AML.CS0010
  name: Microsoft - Azure Service
  object-type: case-study
  summary:
    The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service.
    This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.
  incident-date: 2020-01-01
  procedure:
    - tactic: *id_reconnaissance
      technique: "{{victim_research.id}}"
      description: |
        The team first performed reconnaissance to gather information about the target ML model.
    - tactic: *id_initial_access
      technique: T1078
      description: |
        The team used a valid account to gain access to the network.
    - tactic: *id_collection
      technique: "{{ml_artifact_collection.id}}"
      description: |
        The team found the model file of the target ML model and the necessary training data.
    - tactic: "{{ml_attack_staging.id}}"
      technique: "{{craft_adv_whitebox.id}}"
      description: |
        Using the target model and data, the red team crafted evasive adversarial data.
    - tactic: "{{ml_model_access.id}}"
      technique: "{{inference_api.id}}"
      description: |
        The team used an exposed API to access the target model.
    - tactic: *id_impact
      technique: "{{evade_model.id}}"
      description: |
        The team performed an online evasion attack by replaying the adversarial examples, which helped achieve this goal.
  reported-by:
    - Microsoft (Azure Trustworthy Machine Learning)
  references:

- id: AML.CS0011
  name: Microsoft Edge AI - Evasion
  object-type: case-study
  summary: |
    The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
  incident-date: 2020-02-01
  procedure:
    - tactic: *id_reconnaissance
      technique: "{{victim_research.id}}"
      description: |
        The team first performed reconnaissance to gather information about the target ML model.
    - tactic: *id_resource_development
      technique: "{{acquire_ml_artifacts.id}}"
      description: |
        The team identified and obtained the publically available base model.
    - tactic: *id_resource_development
      technique: "{{acquire_ml_artifacts.id}}"
      description: |
        The team identified and obtained the publically dataset used by the model
    - tactic: "{{ml_model_access.id}}"
      technique: "{{inference_api.id}}"
      description: |
        Then using the publicly available version of the ML model, started sending queries and analyzing the responses (inferences) from the ML model.
    - tactic: "{{ml_attack_staging.id}}"
      technique: "{{craft_adv_blackbox.id}}"
      description: |
        The red team created an automated system that continuously manipulated an original target image, that tricked the ML model into producing incorrect inferences, but the perturbations in the image were unnoticeable to the human eye.
    - tactic: *id_impact
      technique: "{{evade_model.id}}"
      description: |
        Feeding this perturbed image, the red team was able to evade the ML model by causing misclassifications.
  reported-by:
    - Microsoft
  references:

- id: AML.CS0012
  name: MITRE - Physical Adversarial Attack on Face Identification
  object-type: case-study
  summary: |
   MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
   This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
  incident-date: 2020-01-01
  procedure:
    - tactic: *id_reconnaissance
      technique: "{{victim_research.id}}"
      description: |
        The team first performed reconnaissance to gather information about the target ML model.
    - tactic: *id_initial_access
      technique: T1078
      description: |
        The team gained access via a valid account.
    - tactic: "{{ml_model_access.id}}"
      technique: "{{inference_api.id}}"
      description: |
        The team accessed the inference API of the target model.
    - tactic: *id_discovery
      technique: "{{discover_model_ontology.id}}"
      description: |
        The team identified the list of identities targeted by the model by querying the target model's inference API.
    - tactic: *id_resource_development
      technique: "{{acquire_ml_artifacts_data.id}}"
      description: |
        The team acquired representative open source data.
    - tactic: *id_resource_development
      technique: "{{train_proxy_model.id}}"
      description: |
        The team developed a proxy model using the open source data.
    - tactic: "{{ml_attack_staging.id}}"
      technique: "{{craft_adv_whitebox.id}}"
      description: |
        Using the proxy model, the red team optimized a physical domain patch-based attack using expectation over transformation.
    - tactic: "{{ml_model_access.id}}"
      technique: "{{physical_env.id}}"
      description: |
        The team placed the physical countermeasure in the physical environment.
    - tactic: *id_impact
      technique: "{{evade_model.id}}"
      description: |
        The team successfully evaded the model using the physical countermeasure and causing targeted misclassifications.
  reported-by:
    - MITRE AI Red Team
  references:
