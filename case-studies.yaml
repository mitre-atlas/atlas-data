- id: MLCS0000
  object-type: case-study
  name: Evasion of Deep Learning detector for malware C&C traffic
  summary: Palo Alto Networks Security AI research team tested a deep learning model
    for malware command and control (C&C) traffic detection in HTTP traffic. Based
    on the publicly available paper by Le et al. [1] (open source intelligence), we
    built a model that was trained on a similar dataset as our production model and
    had performance similar to it. Then we crafted adversarial samples and queried
    the model and adjusted the adversarial sample accordingly till the model was evaded.
  date: ''
  procedure:
  - tactic: ''
    technique: ''
    description: The team trained the model on ~ 33 million benign and ~ 27 million
      malicious HTTP packet headers
  - tactic: ''
    technique: ''
    description: Evaluation showed a true positive rate of ~ 99% and false positive
      rate of ~0.01%, on average
  - tactic: ''
    technique: ''
    description: Testing the model with a HTTP packet header from known malware command
      and control traffic samples was detected as malicious with high confidence (>
      99%).
  - tactic: ''
    technique: ''
    description: The attackers crafted evasion samples by removing fields from packet
      header which are typically not used for C&C communication (e.g. cache-control,
      connection, etc.)
  - tactic: ''
    technique: ''
    description: With the crafted samples the attackers performed online evasion of
      the ML based spyware detection model. The crafted packets were identified as
      benign with >80% confidence.
  - tactic: ''
    technique: ''
    description: This evaluation demonstrates that adversaries are able to bypass
      advanced ML detection techniques, by crafting samples that are misclassified
      by an ML model.
  reported-by: '- Palo Alto Networks (Network Security AI Research Team)'
  verified-by: ''
  sources: '- [1] Le, Hung, et al. "URLNet: Learning a URL representation with deep
    learning for malicious URL detection." arXiv preprint arXiv:1802.03162 (2018).'
- id: MLCS0001
  object-type: case-study
  name: Botnet Domain Generation Algorithm (DGA) Detection Evasion
  summary: Palo Alto Networks Security AI research team was able to bypass a Convolutional
    Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection
    [1] by domain name mutations. It is a generic domain mutation technique which
    can evade most ML-based DGA detection modules, and can also be used for testing
    against all DGA detection products in the security industry.
  date: ''
  procedure:
  - tactic: ''
    technique: ''
    description: DGA detection is a widely used technique to detect botnets in academia
      and industry.
  - tactic: ''
    technique: ''
    description: The researchers look into a publicly available CNN-based DGA detection
      model [1] and tested against a well-known DGA generated domain name data sets,
      which includes ~50 million domain names from 64 botnet DGA families.
  - tactic: ''
    technique: ''
    description: The CNN-based DGA detection model shows more than 70% detection accuracy
      on 16 (~25%) botnet DGA families.
  - tactic: ''
    technique: ''
    description: On the DGA generated domain names from 16 botnet DGA families, we
      developed a generic mutation technique that requires a minimum number of mutations,
      but achieves a very high evasion rate.
  - tactic: ''
    technique: ''
    description: Experiment results show that, after only one string is inserted once
      to the DGA generated domain names, the detection rate of all 16 botnet DGA families
      can drop to less than 25% detection accuracy.
  - tactic: ''
    technique: ''
    description: The mutation technique can evade almost all DGA detections, not limited
      to CNN-based DGA detection shown in this example. If the attackers add it on
      top of the existing DGA, most of the DGA detections might fail.
  - tactic: ''
    technique: ''
    description: The generic mutation techniques can also be used to test the effectiveness
      and robustness of all DGA detection methods developed by security companies
      in the industry before it is deployed to the production environment.
  reported-by: '- Palo Alto Networks (Network Security AI Research Team)'
  verified-by: ''
  sources: '- [1] Yu, Bin, Jie Pan, Jiaming Hu, Anderson Nascimento, and Martine De
    Cock. "Character level based detection of DGA domain names." In 2018 International
    Joint Conference on Neural Networks (IJCNN), pp. 1-8. IEEE, 2018. Source code
    is available from Github: https://github.com/matthoffman/degas'
- id: MLCS0002
  object-type: case-study
  name: VirusTotal Poisoning
  summary: An increase in reports of a certain ransomware family that was out of the
    ordinary was noticed. In investigating the case, it was observed that many samples
    of that particular ransomware family were submitted through a popular Virus-Sharing
    platform within a short amount of time. Further investigation revealed that based
    on string similarity, the samples were all equivalent, and based on code similarity
    they were between 98 and 74 percent similar. Interestingly enough, the compile
    time was the same for all the samples.  After more digging, the discovery was
    made that someone used 'metame' a metamorphic code manipulating tool to manipulate
    the original file towards mutant variants. The variants wouldn't always be executable
    but still classified as the same ransomware family.
  date: ''
  procedure:
  - tactic: ''
    technique: ''
    description: "The actor used a malware sample from a prevalent ransomware family\
      \ as a start to create \u2018mutant\u2019 variants."
  - tactic: ''
    technique: ''
    description: "The actor uploaded \u2018mutant\u2019 samples to the platform."
  - tactic: ''
    technique: ''
    description: "Several vendors started to classify the files as the ransomware\
      \ family even though most of them won\u2019t run."
  - tactic: ''
    technique: ''
    description: "The \u2018mutant\u2018 samples poisoned the dataset the ML model(s)\
      \ use to identify and classify this ransomware family."
  reported-by: '- Christiaan Beek (@ChristiaanBeek) - McAfee ATR Team'
  verified-by: ''
  sources: '- McAfee Advanced Threat Research'
- id: MLCS0003
  object-type: case-study
  name: Bypassing Cylance's AI Malware Detection
  summary: Researchers at Skylight were able to create a universal bypass string that
    when appended to a malicious file evades detection by Cylance's AI Malware detector.
  date: ''
  procedure:
  - tactic: ''
    technique: ''
    description: The researchers read publicly available information and enabled verbose
      logging to understand the inner workings of the ML model, particularly around
      reputation scoring.
  - tactic: ''
    technique: ''
    description: The researchers reverse-engineered the ML model to understand which
      attributes provided what level of positive or negative reputation. Along the
      way, they discovered a secondary model which was an override for the first model.
      Positive assessments from the second model overrode the decision of the core
      ML model.
  - tactic: ''
    technique: ''
    description: Using this knowledge, the researchers fused attributes of known good
      files with malware. Due to the secondary model overriding the primary, the researchers
      were effectively able to bypass the ML model.
  reported-by: Research and work by Adi Ashkenazy, Shahar Zini, and SkyLight Cyber
    team. Notified to us by Ken Luu (@devianz_)
  verified-by: ''
  sources: '- https://skylightcyber.com/2019/07/18/cylance-i-kill-you/'
- id: MLCS0004
  object-type: case-study
  name: Camera Hijack Attack on Facial Recognition System
  summary: This type of attack can break through the traditional live detection model
    and cause the misuse of face recognition.
  date: ''
  procedure:
  - tactic: ''
    technique: ''
    description: The attackers bought customized low-end mobile phones, customized
      android ROMs, a specific virtual camera application, identity information and
      face photos.
  - tactic: ''
    technique: ''
    description: The attackers used software to turn static photos into videos, adding
      realistic effects such as blinking eyes. Then the attackers use the purchased
      low-end mobile phone to import the generated video into the virtual camera app.
  - tactic: ''
    technique: ''
    description: The attackers registered an account with the victim's identity information.
      In the verification phase, the face recognition system called the camera API,
      but because the system was hooked or rooted, the video stream given to the face
      recognition system was actually provided by the virtual camera app.
  - tactic: ''
    technique: ''
    description: The attackers successfully evaded the face recognition system and
      impersonated the victim.
  reported-by: '- Henry Xuef, Ant Group AISEC Team'
  verified-by: ''
  sources: '- Ant Group AISEC Team'
- id: MLCS0005
  object-type: case-study
  name: Attack on Machine Translation Service - Google Translate, Bing Translator,
    and Systran Translate
  summary: Machine translation services (such as Google Translate, Bing Translator,
    and Systran Translate) provide public-facing UIs and APIs. A research group at
    UC Berkeley utilized these public endpoints to create an "imitation model" with
    near-production, state-of-the-art translation quality. Beyond demonstrating that
    IP can be stolen from a black-box system, they used the imitation model to successfully
    transfer adversarial examples to the real production services. These adversarial
    inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences
    on Google Translate and Systran Translate websites.
  date: ''
  procedure:
  - tactic: ''
    technique: ''
    description: Using published research papers, the researchers gathered similar
      datasets and model architectures that these translation services used
  - tactic: ''
    technique: ''
    description: They abuse a public facing application to query the model and produce
      machine translated sentence pairs as training data
  - tactic: ''
    technique: ''
    description: Using these translated sentence pairs, researchers trained a substitute
      model (model replication)
  - tactic: ''
    technique: ''
    description: The replicated models were used to construct offline adversarial
      examples that successfully transferred to an online evasion attack
  reported-by: '- Work by Eric Wallace, Mitchell Stern, Dawn Song and reported by
    Kenny Song (@helloksong)'
  verified-by: ''
  sources: '- https://arxiv.org/abs/2004.15015

    - https://www.ericswallace.com/imitation'
- id: MLCS0006
  object-type: case-study
  name: ClearviewAI Misconfiguration
  summary: Clearview AI's source code repository, though password protected, was misconfigured
    to allow an arbitrary user to register an account. This allowed an external researcher
    to gain access to a private code repository that contained Clearview AI production
    credentials, keys to cloud storage buckets containing 70K video samples, and copies
    of its applications and Slack tokens. With access to training data, a bad-actor
    has the ability to cause an arbitrary misclassification in the deployed model.
  date: ''
  procedure:
  - tactic: ''
    technique: ''
    description: In this scenario, a security researcher gained initial access to
      via a "Valid Account" that was created through a misconfiguration. No Adversarial
      ML techniques were used.
  - tactic: ''
    technique: ''
    description: These kinds of attacks illustrate that any attempt to secure ML system
      should be on top of "traditional" good cybersecurity hygiene such as locking
      down the system with least privileges, multi-factor authentication and monitoring
      and auditing.
  reported-by: '-   Mossab Hussein (@mossab_hussein)'
  verified-by: ''
  sources: '-   https://techcrunch.com/2020/04/16/clearview-source-code-lapse/amp/

    -   https://gizmodo.com/we-found-clearview-ais-shady-face-recognition-app-1841961772'
- id: MLCS0007
  object-type: case-study
  name: GPT-2 Model Replication
  summary: OpenAI built GPT-2, a powerful natural language model and adopted a staged-release
    process to incrementally release 1.5 Billion parameter model. Before the 1.5B
    parameter model could be released by OpenAI eventually, two ML researchers replicated
    the model and released it to the public. *Note this is an example of model replication
    NOT model model extraction. Here, the attacker is able to recover a functionally
    equivalent model but generally with lower fidelity than the original model, perhaps
    to do reconnaissance (See ProofPoint attack). In Model extraction, the fidelity
    of the model is comparable to the original, victim model.*
  date: ''
  procedure:
  - tactic: ''
    technique: ''
    description: Using public documentation about GPT-2, ML researchers gathered similar
      datasets used during the original GPT-2 training.
  - tactic: ''
    technique: ''
    description: Next, they used a different publicly available NLP model (called
      Grover) and modified Grover's objective function to reflect
  - tactic: ''
    technique: ''
    description: PT-2's objective function.
  - tactic: ''
    technique: ''
    description: The researchers then trained the modified Grover on the dataset they
      curated, using Grover's initial hyperparameters, which
  - tactic: ''
    technique: ''
    description: resulted in their replicated model.
  reported-by: '- Vanya Cohen (@VanyaCohen)

    - Aaron Gokaslan (@SkyLi0n)

    - Ellie Pavlick

    - Stefanie Tellex'
  verified-by: ''
  sources: '- https://www.wired.com/story/dangerous-ai-open-source/

    - https://blog.usejournal.com/opengpt-2-we-replicated-gpt-2-because-you-can-too-45e34e6d36dc'
- id: MLCS0008
  object-type: case-study
  name: ProofPoint Evasion
  summary: CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection
    system by first building a copy-cat email protection ML model, and using the insights
    to evade the live system.
  date: ''
  procedure:
  - tactic: ''
    technique: ''
    description: The researchers first gathered the scores from the Proofpoint's ML
      system used in email email headers.
  - tactic: ''
    technique: ''
    description: Using these scores, the researchers replicated the ML mode by building
      a "shadow" aka copy-cat ML model.
  - tactic: ''
    technique: ''
    description: Next, the ML researchers algorithmically found samples that this
      "offline" copy cat model.
  - tactic: ''
    technique: ''
    description: Finally, these insights from the offline model allowed the researchers
      to create malicious emails that received preferable scores from the real ProofPoint
      email protection system, hence bypassing it.
  reported-by: '- Will Pearce (@moo_hax)

    - Nick Landers (@monoxgas)'
  verified-by: ''
  sources: '- https://nvd.nist.gov/vuln/detail/CVE-2019-20634

    - https://github.com/moohax/Talks/blob/master/slides/DerbyCon19.pdf

    - https://github.com/moohax/Proof-Pudding'
- id: MLCS0009
  object-type: case-study
  name: Tay Poisoning
  summary: Microsoft created Tay, a twitter chatbot for 18- to 24- year-olds in the
    U.S. for entertainment purposes. Within 24 hours of its deployment, Tay had to
    be decommissioned because it tweeted reprehensible words.
  date: ''
  procedure:
  - tactic: ''
    technique: ''
    description: Tay bot used the interactions with its twitter users as training
      data to improve its conversations.
  - tactic: ''
    technique: ''
    description: Average users of Twitter coordinated together with the intent of
      defacing Tay bot by exploiting this feedback loop.
  - tactic: ''
    technique: ''
    description: As a result of this coordinated attack, Tay's training data was poisoned
      which led its conversation algorithms to generate more reprehensible material.
  verified-by: ''
  sources: '- https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/

    - https://spectrum.ieee.org/tech-talk/artificial-intelligence/machine-learning/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation'
- id: MLCS0010
  object-type: case-study
  name: Microsoft - Azure Service
  summary: The Azure Red Team and Azure Trustworthy ML team performed a red team exercise
    on an internal Azure service with the intention of disrupting its service.
  date: ''
  procedure:
  - tactic: ''
    technique: ''
    description: The team first performed reconnaissance to gather information about
      the target ML model.
  - tactic: ''
    technique: ''
    description: Then, using a valid account the team found the model file of the
      target ML model and the necessary training data.
  - tactic: ''
    technique: ''
    description: Using this, the red team performed an offline evasion attack by methodically
      searching for adversarial examples.
  - tactic: ''
    technique: ''
    description: Via an exposed API interface, the team performed an online evasion
      attack by replaying the adversarial examples, which helped achieve this goal.
  - tactic: ''
    technique: ''
    description: This operation had a combination of traditional ATT&CK enterprise
      techniques such as finding Valid account, and Executing code via an API -- all
      interleaved with adversarial ML specific steps such as offline and online evasion
      examples.
  reported-by: '- Microsoft (Azure Trustworthy Machine Learning)'
  verified-by: ''
  sources: '- None'
- id: MLCS0011
  object-type: case-study
  name: Microsoft - Edge AI
  summary: The Azure Red Team performed a red team exercise on a new Microsoft product
    designed for running AI workloads at the Edge.
  date: ''
  procedure:
  - tactic: ''
    technique: ''
    description: The team first performed reconnaissance to gather information about
      the target ML model.
  - tactic: ''
    technique: ''
    description: Then, used a publicly available version of the ML model, started
      sending queries and analyzing the responses (inferences) from the ML model.
  - tactic: ''
    technique: ''
    description: Using this, the red team created an automated system that continuously
      manipulated an original target image, that tricked the ML model into producing
      incorrect inferences, but the perturbations in the image were unnoticeable to
      the human eye.
  - tactic: ''
    technique: ''
    description: Feeding this perturbed image, the red team was able to evade the
      ML model into misclassifying the input image.
  - tactic: ''
    technique: ''
    description: This operation had one step in the traditional MITRE ATT&CK techniques
      to do reconnaissance on the ML model being used in the product, and then the
      rest of the techniques was to use Offline evasion, followed by online evasion
      of the targeted product.
  reported-by: Microsoft
  verified-by: ''
  sources: None
- id: MLCS0012
  object-type: case-study
  name: MITRE - Physical Adversarial Attack on Face Identification
  summary: "MITRE\u2019s AI Red Team demonstrated a physical-domain evasion attack\
    \ on a commercial face identification service with the intention of inducing a\
    \ targeted misclassification."
  date: ''
  procedure:
  - tactic: ''
    technique: ''
    description: The team first performed reconnaissance to gather information about
      the target ML model.
  - tactic: ''
    technique: ''
    description: Using a valid account, the team identified the list of IDs targeted
      by the model.
  - tactic: ''
    technique: ''
    description: The team developed a proxy model using open source data.
  - tactic: ''
    technique: ''
    description: Using the proxy model, the red team optimized a physical domain patch-based
      attack using an expectation of transformations.
  - tactic: ''
    technique: ''
    description: Via an exposed API interface, the team performed an online physical-domain
      evasion attack including the adversarial patch in the input stream which resulted
      in a targeted misclassification.
  - tactic: ''
    technique: ''
    description: "This operation had a combination of traditional ATT&CK enterprise\
      \ techniques such as finding Valid account, and Executing code via an API \u2013\
      \ all interleaved with adversarial ML specific attacks."
  reported-by: MITRE AI Red Team
  verified-by: ''
  sources: None
